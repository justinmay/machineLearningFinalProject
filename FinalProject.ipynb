{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning for Engineers Final Project\n",
    "#### by: Justin May, Jonathan Hong, and Joseph Shenouda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Universal imports\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import csv\n",
    "import pickle\n",
    "import time\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Handy function to monitor progress of running cells\n",
    "\"\"\"\n",
    "def progress(count, total, suffix=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', suffix))\n",
    "    sys.stdout.flush()  # As suggested by Rom Ruben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imaging Dataset Classification Problem\n",
    "[CIFAR-10 Dataset](https://www.cs.toronto.edu/~kriz/cifar.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing as sklpp\n",
    "from sklearn import decomposition as skldecomp\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting file images/data_batch_1...\n",
      "Finished Extracting Features\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extracting data from file into numpy array\n",
    "\"\"\"\n",
    "data = None\n",
    "labels = []\n",
    "\n",
    "file = \"images/data_batch_1\"\n",
    "with open(file, 'rb') as fo:\n",
    "    print(\"extracting file \"+file+\"...\")\n",
    "    dict = pickle.load(fo, encoding='bytes')\n",
    "    temp_data = dict[b'data']\n",
    "    try:\n",
    "        data = np.concatenate((data, temp_data), axis=0)\n",
    "    except:\n",
    "        data = temp_data\n",
    "    labels = labels + dict[b'labels']\n",
    "\n",
    "labels = np.asarray(labels)\n",
    "data = np.asarray(data)\n",
    "labels = labels.reshape(-1,1)\n",
    "print(\"Finished Extracting Features\")\n",
    "print(np.shape(data))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Learning For Images Dataset\n",
    "For preprocessing we centered our dataset to 0 mean and then for feature learning we apply PCA to reduce our dimensions from 3072 to a smaller number of features than contains 95% of the variance of our data. We did this in order to speed up our computation time for the other algorithms we would be implementing on the dataset for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a StandardScaler object to 0 mean the data matrix but preserve the variance\n",
    "stand_scaler = sklpp.StandardScaler(with_mean = True, with_std = False)\n",
    "# Fits the data matrix to the StandardScaler object defined ^\n",
    "centered_ImageData = stand_scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a PCA object that reduces the dimensions of our data matrix keeping 95% of the variance\n",
    "pca_obj = skldecomp.PCA(n_components = 0.95, svd_solver = 'auto')\n",
    "dim_reducedImageData = pca_obj.fit_transform(centered_ImageData)\n",
    "np.save('dim_reducedImageData.npy', dim_reducedImageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been reduced to 209 features after PCA\n"
     ]
    }
   ],
   "source": [
    "print('Data has been reduced to {} features after PCA'.format(dim_reducedImageData.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis (by: Justin May)\n",
    "I implemeneted the algorithim we learned in class:\n",
    "\n",
    "first learn the parameters (expected average, prior probabilities, covariance matricies)\n",
    "define the discriminant functions to find the best labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...training k= 1\n",
      "0.466========================================================] 100.0% ...testing k= 1\n",
      "[============================================================] 100.0% ...training k= 2\n",
      "0.498========================================================] 100.0% ...testing k= 2\n",
      "[============================================================] 100.0% ...training k= 3\n",
      "0.479========================================================] 100.0% ...testing k= 3\n",
      "[============================================================] 100.0% ...training k= 4\n",
      "0.473========================================================] 100.0% ...testing k= 4\n",
      "[============================================================] 100.0% ...training k= 5\n",
      "0.464========================================================] 100.0% ...testing k= 5\n",
      "[============================================================] 100.0% ...training k= 6\n",
      "0.49=========================================================] 100.0% ...testing k= 6\n",
      "[============================================================] 100.0% ...training k= 7\n",
      "0.441========================================================] 100.0% ...testing k= 7\n",
      "[============================================================] 100.0% ...training k= 8\n",
      "0.469========================================================] 100.0% ...testing k= 8\n",
      "[============================================================] 100.0% ...training k= 9\n",
      "0.482========================================================] 100.0% ...testing k= 9\n",
      "[============================================================] 100.0% ...training k= 10\n",
      "0.473========================================================] 100.0% ...testing k= 10\n",
      "\n",
      "[0.466, 0.498, 0.479, 0.473, 0.464, 0.49, 0.441, 0.469, 0.482, 0.473]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pull pca data from saved\n",
    "\"\"\"\n",
    "image_data = np.load('dim_reducedImageData.npy')\n",
    "image_data = np.concatenate((image_data,labels),axis=1)\n",
    "number_of_parameters = len(image_data[0]) - 1 #209\n",
    "number_of_total_samples = len(image_data)\n",
    "\"\"\"\n",
    "K-Fold cross validation \n",
    "\"\"\"     \n",
    "final_estimates = []\n",
    "k = 10\n",
    "for i in range(1,k+1):\n",
    "    training_data = np.concatenate((image_data[0:(i-1)*int(number_of_total_samples/k)],image_data[i*int(number_of_total_samples/k):number_of_total_samples]),axis=0)\n",
    "    testing_data = image_data[(i-1)*int(number_of_total_samples/k):i*int(number_of_total_samples/k)]\n",
    "    number_of_training_samples = len(training_data)\n",
    "    \"\"\"\n",
    "    Define Parameters \n",
    "    \"\"\"\n",
    "    mu = [[0 for j in range(number_of_parameters)] for i in range(10)] # initializing means \n",
    "    sigmas = [[[0 for k in range(number_of_parameters)] for j in range(number_of_parameters)] for i in range(10)] # initializing variances\n",
    "    occurences = [0 for i in range(10)]\n",
    "    pi = [0 for i in range(10)]\n",
    "        \n",
    "    \"\"\"\n",
    "    Learn mu\n",
    "    \"\"\"\n",
    "    # Find Sums and Occurences\n",
    "    for image in training_data:\n",
    "        label = int(image[-1])\n",
    "        occurences[label] += 1\n",
    "        mu[label] = np.add(mu[label],image[:-1])\n",
    "    # Calculate Averages and Prior Probabilities \n",
    "    for label,sums in enumerate(mu):\n",
    "        mu[label] = np.multiply(1/(occurences[label] - 1), sums).reshape(-1,1) #unbaised estimator\n",
    "        pi[label] = occurences[label] / number_of_total_samples\n",
    "    \"\"\"\n",
    "    Learn sigma^2\n",
    "    \"\"\"\n",
    "    time = 0\n",
    "    for image in training_data:\n",
    "        time += 1\n",
    "        if time%10 == 0:\n",
    "            progress(time,number_of_training_samples,suffix=\"training k= \"+str(i))\n",
    "        label = int(image[-1])\n",
    "        image = image[:-1].reshape(-1,1)\n",
    "        difference = np.subtract(image,mu[label])\n",
    "        sigma = difference.dot(difference.T)\n",
    "        sigmas[label] = np.add(sigmas[label],sigma)\n",
    "    for label,sigma in enumerate(sigmas):\n",
    "        sigmas[label] = np.multiply(1/(occurences[label]-1),sigma)\n",
    "    sys.stdout.flush()\n",
    "    \"\"\"\n",
    "    Method to find the best discriminant score\n",
    "    \"\"\"\n",
    "    def estimateBestLabel(image):\n",
    "        scores = [0 for _ in range(10)]\n",
    "        # find score for each label \n",
    "\n",
    "        for label in range(10):\n",
    "            inverted_variance = np.linalg.inv(sigmas[label])\n",
    "            first_term = -0.5*image.T.dot(inverted_variance).dot(image)\n",
    "            second_term = image.T.dot(inverted_variance).dot(mu[label])\n",
    "            third_term = -0.5*mu[label].T.dot(inverted_variance).dot(mu[label])\n",
    "            (sign, logdet) = np.linalg.slogdet(sigmas[label])\n",
    "            fourth_term = -0.5*sign*logdet\n",
    "            fifth_term = math.log(pi[label])\n",
    "            score = first_term + second_term + third_term + fourth_term + fifth_term\n",
    "            scores[label] = score[0][0]\n",
    "        return(scores.index(max(scores)))\n",
    "    correct = 0\n",
    "    print(\"\")\n",
    "    for index, image in enumerate(testing_data):\n",
    "        if index % 9 == 0:\n",
    "            progress(index+1,1000,suffix=\"testing k= \"+str(i))\n",
    "        image = image[:-1].reshape(-1,1)\n",
    "        if testing_data[index][-1] == estimateBestLabel(image):\n",
    "            correct += 1\n",
    "    sys.stdout.flush()\n",
    "    print(correct/1000)\n",
    "    final_estimates.append(correct/1000)\n",
    "print(\"\")\n",
    "print(final_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4734999999999999"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_estimatesAverage = sum([0.466, 0.498, 0.479, 0.473, 0.464, 0.49, 0.441, 0.469, 0.482, 0.473])/10\n",
    "final_estimatesAverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[============================================================] 100.0% ...testing k= 10\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Method to find the best discriminant score\n",
    "\"\"\"\n",
    "def estimateBestLabel(image):\n",
    "    scores = [0 for _ in range(10)]\n",
    "    # find score for each label \n",
    "\n",
    "    for label in range(10):\n",
    "        inverted_variance = np.linalg.inv(sigmas[label])\n",
    "        first_term = -0.5*image.T.dot(inverted_variance).dot(image)\n",
    "        second_term = image.T.dot(inverted_variance).dot(mu[label])\n",
    "        third_term = -0.5*mu[label].T.dot(inverted_variance).dot(mu[label])\n",
    "        (sign, logdet) = np.linalg.slogdet(sigmas[label])\n",
    "        fourth_term = -0.5*sign*logdet\n",
    "        fifth_term = math.log(pi[label])\n",
    "        score = first_term + second_term + third_term + fourth_term + fifth_term\n",
    "        scores[label] = score[0][0]\n",
    "    return(scores.index(max(scores)))\n",
    "correct = 0\n",
    "print(\"\")\n",
    "estimated = []\n",
    "real = []\n",
    "for index, image in enumerate(testing_data):\n",
    "    if index % 9 == 0:\n",
    "        progress(index+1,1000,suffix=\"testing k= \"+str(i))\n",
    "    image = image[:-1].reshape(-1,1)\n",
    "    real_label = testing_data[index][-1]\n",
    "    estimated_label = estimateBestLabel(image)\n",
    "    real.append(real_label)\n",
    "    estimated.append(estimated_label)\n",
    "sys.stdout.flush()\n",
    "confusion = confusion_matrix(real, estimated,labels=[0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             airplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n",
      "   airplane        45           7     4    5     5    3     0      3    16     16\n",
      " automobile         4          70     1    3     0    0     2      0     1     26\n",
      "       bird         8           2    32   16    23   15     5      4     1      9\n",
      "        cat         2           6     4   42    10   18     6     12     1      8\n",
      "       deer         8           8     9    4    44    4     0     16     3      7\n",
      "        dog         1           4     7   13     1   26     4     11     0     10\n",
      "       frog         0          10     8   12     7    6    50      1     5      9\n",
      "      horse         4           6     2    4     5    6     2     49     0      8\n",
      "       ship        13          13     0    0     4    0     2      0    57     12\n",
      "      truck         5          16     0    2     0    1     0      2     6     58\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Print Confusion Matrix \n",
    "\"\"\"\n",
    "image_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship' ,'truck']\n",
    "confusion = pd.DataFrame(confusion)\n",
    "confusion.insert(0, \"\", image_labels, True)\n",
    "confusion.columns = [\"\", 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship' ,'truck']\n",
    "print(confusion.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbor Classifier(by: Joseph Shenouda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import neighbors as sklneighb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbors with SciKit Learn\n",
    "We first create a KNeighborsClassifier that accepts the following parameters\n",
    "1. **metric**: this defines the method we will use to calculate the distance between a test sample and the training samples to determine the K nearest training samples to the given test\n",
    "2. **algorithm**: this is set to 'auto' and it tells SciKit which algorithm it should use to calculate the nearest neighbors we left it to the default of auto so that SciKit would give the best choice based on our training data\n",
    "3. **leaf_size**: Leaf_size is a parameter used for 2 of the possible algorithms from the algorithms parameter BallTree and KDTree we left this at the default because our algorithm was being choosen by SciKit\n",
    "4. **p** : p is the power parameter for the Minkowski metric in our case since we are doing euclidean distance it will just be 2\n",
    "5. **weights**: is set to its default value of 'uniform' so that each point in the neighborhood of test sample has the same strength and contributes as equally as any other point to the overall classification\n",
    "6. **metric_params**: additional keyword arguments for the distance metric used\n",
    "7. **n_jobs**: number of parallel jobs we set it to the defualt of none because we are not doing parallel\n",
    "\n",
    "Then we fit the object we just created to our training data and finally we call the KNeighborsClassifier.predict method which accepts the test samples as inputs and returns the labels it classified each sample as it does this in the following steps:\n",
    "1. It finds the n_neighbors closest training points using the metric defined for the KNeighborsClassifier we defined earlier\n",
    "2. Then based on the closest n_neighbors it found it assigns the test sample to the label that turned up the most in the n_neighbors closest training samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "A method to compute the kth fold CV of K nearest neighbor\n",
    "Parameters:\n",
    "k_iterCV - The current iteration of k fold CV\n",
    "k_NN -  The number of neighbors for K Nearest Neighbors algo\n",
    "data - The data matrix to be split into training and test\n",
    "kCVResults - An array that stores the results of each K-Fold CV result\n",
    "\"\"\"\n",
    "def kFoldKNN(k_iterCV, k_NN, data, kFoldCVResults, kFolds):\n",
    "    number_of_total_samples = data.shape[0]\n",
    "    \n",
    "    #Splits up the data ito training and testing data for each iteration of K-fold CV\n",
    "    training_data = np.concatenate((data[0:(k_iterCV-1)*int(number_of_total_samples/kFolds)],data[k_iterCV*int(number_of_total_samples/kFolds):number_of_total_samples]),axis=0)\n",
    "    training_labels = np.concatenate((labels[0:(k_iterCV-1)*int(number_of_total_samples/kFolds)],labels[k_iterCV*int(number_of_total_samples/kFolds):number_of_total_samples]),axis=0)\n",
    "    \n",
    "    testing_data = data[(k_iterCV-1)*int(number_of_total_samples/kFolds):k_iterCV*int(number_of_total_samples/kFolds)]\n",
    "    test_data_labels = labels[(k_iterCV-1)*int(number_of_total_samples/kFolds):k_iterCV*int(number_of_total_samples/kFolds)]\n",
    "    \n",
    "    # KNN implementation from SciKit Learn as explained above \n",
    "    neigh = sklneighb.KNeighborsClassifier(metric='euclidean',n_neighbors = k_NN)\n",
    "    neigh.fit(training_data,training_labels)\n",
    "    KNNClassifierResults = neigh.predict(testing_data)\n",
    "    \n",
    "    # From the SciKit learn library this method takes in the ground truth labels of the test data and the predicted labels then returns\n",
    "    # the fraction of correctly classified samples\n",
    "    kFoldCVResults.append(metrics.accuracy_score(test_data_labels,KNNClassifierResults)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 different K values for KNN\n",
    "kNNs = [5, 15 , 20, 30, 50]\n",
    "\n",
    "#A dictionary that stores the K for KNN as keys\n",
    "#And the list of kFoldCVResults for that KNN as values\n",
    "kNNResults = {}\n",
    "\n",
    "#Iterating over each K value for KNN\n",
    "for kNN in kNNs:\n",
    "    kFoldResults = []\n",
    "    #10-fold Cross validation\n",
    "    for k in range(1,11):\n",
    "        kFoldKNN(k,kNN, dim_reducedImageData, kFoldResults, 10)\n",
    "    kNNResults[kNN] = kFoldResults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average error after 10-Fold CV for k = 5 KNN is 0.3043\n",
      "\n",
      "The average error after 10-Fold CV for k = 15 KNN is 0.30260000000000004\n",
      "\n",
      "The average error after 10-Fold CV for k = 20 KNN is 0.3045\n",
      "\n",
      "The average error after 10-Fold CV for k = 30 KNN is 0.29900000000000004\n",
      "\n",
      "The average error after 10-Fold CV for k = 50 KNN is 0.2942\n",
      "\n"
     ]
    }
   ],
   "source": [
    "avgError_kNN5 = sum(kNNResults[5])/10\n",
    "avgError_kNN15 = sum(kNNResults[15])/10\n",
    "avgError_kNN20 = sum(kNNResults[20])/10\n",
    "avgError_kNN30 = sum(kNNResults[30])/10\n",
    "avgError_kNN50 = sum(kNNResults[50])/10\n",
    "\n",
    "print('The average error after 10-Fold CV for k = 5 KNN is {}\\n'.format(avgError_kNN5))\n",
    "print('The average error after 10-Fold CV for k = 15 KNN is {}\\n'.format(avgError_kNN15))\n",
    "print('The average error after 10-Fold CV for k = 20 KNN is {}\\n'.format(avgError_kNN20))\n",
    "print('The average error after 10-Fold CV for k = 30 KNN is {}\\n'.format(avgError_kNN30))\n",
    "print('The average error after 10-Fold CV for k = 50 KNN is {}\\n'.format(avgError_kNN50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (by Jonathan Hong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reducedImageData = np.load('dim_reducedImageData.npy')\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=False)\n",
    "labels = labels.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Iteration  1 : \n",
      "\n",
      "\tTraining set: [ 1000 ... 9999 ] \tTest set: [ 0 ... 999 ]\n",
      "Score:  0.379\n",
      "Training Iteration  2 : \n",
      "\n",
      "\tTraining set: [ 0 ... 9999 ] \tTest set: [ 1000 ... 1999 ]\n",
      "Score:  0.379\n",
      "Training Iteration  3 : \n",
      "\n",
      "\tTraining set: [ 0 ... 9999 ] \tTest set: [ 2000 ... 2999 ]\n",
      "Score:  0.384\n",
      "Training Iteration  4 : \n",
      "\n",
      "\tTraining set: [ 0 ... 9999 ] \tTest set: [ 3000 ... 3999 ]\n",
      "Score:  0.367\n",
      "Training Iteration  5 : \n",
      "\n",
      "\tTraining set: [ 0 ... 9999 ] \tTest set: [ 4000 ... 4999 ]\n",
      "Score:  0.348\n",
      "Training Iteration  6 : \n",
      "\n",
      "\tTraining set: [ 0 ... 9999 ] \tTest set: [ 5000 ... 5999 ]\n",
      "Score:  0.398\n",
      "Training Iteration  7 : \n",
      "\n",
      "\tTraining set: [ 0 ... 9999 ] \tTest set: [ 6000 ... 6999 ]\n",
      "Score:  0.386\n",
      "Training Iteration  8 : \n",
      "\n",
      "\tTraining set: [ 0 ... 9999 ] \tTest set: [ 7000 ... 7999 ]\n",
      "Score:  0.374\n",
      "Training Iteration  9 : \n",
      "\n",
      "\tTraining set: [ 0 ... 9999 ] \tTest set: [ 8000 ... 8999 ]\n",
      "Score:  0.378\n",
      "Training Iteration  10 : \n",
      "\n",
      "\tTraining set: [ 0 ... 8999 ] \tTest set: [ 9000 ... 9999 ]\n",
      "Score:  0.386\n",
      "Accuracies: [0.379, 0.379, 0.384, 0.367, 0.348, 0.398, 0.386, 0.374, 0.378, 0.386]\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "accuracy = []\n",
    "models = []\n",
    "count = 1\n",
    "\n",
    "# split into training and validation index\n",
    "for train_index, test_index in kf.split(dim_reducedImageData):\n",
    "    print(\"Training Iteration \", count, \": \\n\")\n",
    "    print(\"\\tTraining set: [\", train_index[0], '...', train_index[-1], \"] \\tTest set: [\", test_index[0], '...', test_index[-1], \"]\")\n",
    "    \n",
    "    # split into training and validation dataset\n",
    "    X_train, X_test = dim_reducedImageData[train_index], dim_reducedImageData[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # Use Logistic Regression Model - One vs. All because we need to learn 10 classes\n",
    "    model = LogisticRegression(solver='liblinear', multi_class=\"ovr\", max_iter = 400)\n",
    "    \n",
    "    # fit training data to model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # report the score using the test dataset\n",
    "    score = model.score(X_test, y_test)\n",
    "    \n",
    "    # append the results for later\n",
    "    accuracy.append(score)\n",
    "    models.append(model)\n",
    "    print(\"Score: \", score)\n",
    "    count +=1\n",
    "# results    \n",
    "print('Accuracies:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3779"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logAccuracy_avg = sum([0.379, 0.379, 0.384, 0.367, 0.348, 0.398, 0.386, 0.374, 0.378, 0.386])/10\n",
    "logAccuracy_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Iteration @ K=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             airplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n",
      "   airplane        49           3     5    5     2    1     1      6    25      8\n",
      " automobile         7          36     3    6     4    5     7      3    11      9\n",
      "       bird        11           6    29    8     9    9    14      5     7      2\n",
      "        cat         6           2    11   20     8   19     8      6     7      7\n",
      "       deer         3           4    18    7    32    4    11      9     2      2\n",
      "        dog         2           5    12   18     5   29     7      4     7      6\n",
      "       frog         1           8     8   16     6   10    41      3     2      2\n",
      "      horse         4           4     5    7    10    6     4     58     6      7\n",
      "       ship        15           9     2    2     3    5     1      4    52      7\n",
      "      truck         8          21     2    6     2    3     4      5    12     52\n"
     ]
    }
   ],
   "source": [
    "indexes = np.arange(10000)\n",
    "test_index = np.arange(5000,6000)\n",
    "model = models[5]\n",
    "predictions = model.predict(dim_reducedImageData[test_index])\n",
    "confusion = confusion_matrix(labels[test_index], predictions, labels=[0,1,2,3,4,5,6,7,8,9])\n",
    "image_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship' ,'truck']\n",
    "confusion = pd.DataFrame(confusion)\n",
    "confusion.insert(0, \"\", image_labels, True)\n",
    "confusion.columns = [\"\", 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship' ,'truck']\n",
    "print(confusion.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### QDA\n",
    "From QDA we saw that after cross validation we get accuracy of about 47.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average K-NN results\n",
    "- The average correct classifications after 10-Fold CV for k = 5 KNN is 0.3043\n",
    "- The average correct classifications after 10-Fold CV for k = 15 KNN is 0.30260000000000004\n",
    "- The average correct classifications after 10-Fold CV for k = 20 KNN is 0.3045\n",
    "- The average correct classifications after 10-Fold CV for k = 30 KNN is 0.29900000000000004\n",
    "- The average correct classifications after 10-Fold CV for k = 50 KNN is 0.2942\n",
    "\n",
    "The best K-NN which was 15-NN got us accuracy of only 30.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression Results\n",
    "From Logistic Regression on average we got an accuracy of 37.79%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Therefore from our results we see that the best model for this dataset and problem was Quadratic Discriminant Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Dataset Classification \n",
    "[Tweets for Sentiment Analysis](https://www.kaggle.com/imrandude/twitter-sentiment-analysis?fbclid=IwAR3g6NIHi9alcwDH3BI_qedUknq4xqAf-O7yfy1gMzUAQwQOoanOTK1p5zg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Step\n",
    "Our twitter data preprocessing step consistions of the following:\n",
    "\n",
    "- Tokenizing words the tweets\n",
    "- Removing any links, @ mentions, and #tags that don't have semantic meaning outside of twitter\n",
    "- Lemmatization: remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma\n",
    "- Lower case all words\n",
    "- Remove stopwords: a commonly used word (such as “the”, “a”, “an”, “in”) that have little semantic meaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading in Data\n",
    "\"\"\"\n",
    "df_raw_twitter_data = pd.read_csv('train.csv', header=None, encoding = \"ISO-8859-1\").values[1:]\n",
    "df_raw_twitter_data = df_raw_twitter_data[:,1:] # getting rid of index \n",
    "\n",
    "labels = df_raw_twitter_data[:,0].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 2)\n",
      "\n",
      "[============================================================] 99.9% ...running twitter processing\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Preprocessing\n",
    "\"\"\"\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "print(np.shape(df_raw_twitter_data))\n",
    "corpus = []\n",
    "print(\"\")\n",
    "time = 0\n",
    "for index,[sentiment, tweet] in enumerate(df_raw_twitter_data):\n",
    "    time += 1\n",
    "    # Tokenize Words \n",
    "    tokens = tweet.split(\" \")\n",
    "    # Remove Links, @ mentions, # tags\n",
    "    links = ['http', '.com', '#', '@', '&', '~']\n",
    "    tokens = [w for w in tokens if not any(x in w for x in links)]\n",
    "    # Tokenize again \n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    tokens = [regex.sub('', w) for w in tokens]\n",
    "    # Lemmatization \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens ]\n",
    "    # Clean Up\n",
    "    tokens = [w.lower() for w in tokens if len(w) > 0]\n",
    "    # Remove Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('im')\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    corpus.append(\" \".join(tokens))\n",
    "    if time % 100 == 0:\n",
    "        progress(time,99988,suffix=\"running twitter processing\")\n",
    "sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('corpus.npy', corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished vectorization\n",
      "(5000, 7882)\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "star = 5000\n",
    "##############\n",
    "corpus = np.load('corpus.npy')\n",
    "vectorizer = CountVectorizer()\n",
    "vobj = vectorizer.fit_transform(corpus[:star])\n",
    "vectors = vobj.toarray()\n",
    "print(\"\\nFinished vectorization\")\n",
    "print(np.shape(vectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a StandardScaler object to 0 mean the data matrix but preserve the variance\n",
    "stand_scaler = sklpp.StandardScaler(with_mean = True, with_std = False)\n",
    "# Fits the data matrix to the StandardScaler object defined ^\n",
    "centered_textData = stand_scaler.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a PCA object that reduces the dimensions of our data matrix keeping 90% of the variance\n",
    "pca_obj = skldecomp.PCA(n_components = 0.9, svd_solver = 'auto')\n",
    "dim_reducedTextData = pca_obj.fit_transform(centered_textData)\n",
    "np.save('twitterDataReduced.npy', dim_reducedTextData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been reduced to 1719 features after PCA\n"
     ]
    }
   ],
   "source": [
    "print('Data has been reduced to {} features after PCA'.format(dim_reducedTextData.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines (by Joseph Shenouda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines aim to find a hyperplane in our $p$ dimensional space that maximizes the margin between the hyperplane and the data points closest to it on both sides of the linear boundary mathmatically it solves this optimization problem:\n",
    "$$\\underset{(w,b)}{\\arg\\min}\\frac{1}{n}\\sum_{i=1}^{n}max[0, 1-y_{i}(w^{T}x_{i}+b)]+\\frac{\\lambda}{2}\\|w\\|^{2}$$ here $y\\in\\{-1,1\\}$ and where $\\lambda$ is a regularization parameter that effects the energy of $w$ vector decreasing it as $\\lambda$ increases and increasing the energy of $w$ as $\\lambda$ decreases. In SciKit Learn they use a similar formulation for the support vector machines which is:\n",
    "$$\\begin{align}\\begin{aligned}\\min_ {w, b, \\zeta} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} \\zeta_i\\\\\\begin{split}\\textrm {subject to } & y_i (w^T \\phi (x_i) + b) \\geq 1 - \\zeta_i,\\\\\n",
    "& \\zeta_i \\geq 0, i=1, ..., n\\end{split}\\end{aligned}\\end{align}$$ here the regularization parameter is now C. If C is lower then the summation can be higher meaning it allows for more errors but if C is high then it ensures that the summation must decrease and thus reduce the amount of missclassifications by the hyper-plane.\n",
    "\n",
    "SciKit learn uses the LibLinear library for large linear classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing SVM from SciKit Learn\n",
    "\"\"\"\n",
    "from sklearn import svm as sklSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('twitterDataReduced.npy')\n",
    "labels = labels[:5000]\n",
    "number_of_parameters = len(data[0]) - 1 #5000\n",
    "number_of_total_samples = len(data)\n",
    "number_of_labels = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The regularization parameter (C) being used is 0.001\n",
      "The average # of correct classifications is 0.7112\n",
      "\n",
      "The regularization parameter (C) being used is 0.1\n",
      "The average # of correct classifications is 0.7592000000000001\n",
      "\n",
      "The regularization parameter (C) being used is 1\n",
      "The average # of correct classifications is 0.7308\n",
      "\n",
      "The regularization parameter (C) being used is 5\n",
      "The average # of correct classifications is 0.7152000000000001\n",
      "\n",
      "The regularization parameter (C) being used is 10\n",
      "The average # of correct classifications is 0.7078\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Validation step\n",
    "\"\"\"\n",
    "kFolds = 10\n",
    "reg_params = [0.001, 0.1, 1, 5 ,10]\n",
    "\n",
    "regParamResults = {}\n",
    "\n",
    "for c in reg_params:\n",
    "    validation_results = []\n",
    "    for k_iterCV in range(2,12):\n",
    "        number_of_total_samples = data.shape[0]\n",
    "\n",
    "        fold_size = int(number_of_total_samples/kFolds)\n",
    "\n",
    "        #Splits up the data into training and testing data for each iteration of K-fold CV\n",
    "        if k_iterCV != 11:\n",
    "            training_data = np.concatenate((data[0:(k_iterCV-2)*fold_size],data[k_iterCV*fold_size:number_of_total_samples]),axis=0)\n",
    "            training_labels = np.concatenate((labels[0:(k_iterCV-2)*fold_size],labels[k_iterCV*fold_size:number_of_total_samples]),axis=0)\n",
    "        else:\n",
    "            training_data = data[fold_size:number_of_total_samples-fold_size]\n",
    "            training_labels = labels[fold_size:number_of_total_samples-fold_size]\n",
    "\n",
    "\n",
    "        testing_data = data[(k_iterCV-2)*fold_size:(k_iterCV-1)*fold_size]\n",
    "        test_labels = labels[(k_iterCV-2)*fold_size:(k_iterCV-1)*fold_size]\n",
    "\n",
    "        if k_iterCV != 11:\n",
    "            validation_data = data[(k_iterCV-1)*fold_size:k_iterCV*fold_size]\n",
    "            validation_labels = labels[(k_iterCV-1)*fold_size:k_iterCV*fold_size]\n",
    "        else:\n",
    "            validation_data = data[0:fold_size]\n",
    "            validation_labels = labels[0:fold_size]\n",
    "            \n",
    "        # fit_intercept set to false because the data was already centered from PCA\n",
    "        # dual set to false to get rid of ConvergenceWarnings\n",
    "        svm_obj = sklSVM.LinearSVC(C = c, dual = False)\n",
    "\n",
    "        #ravel converts narray to (n,) shape which SVM uses\n",
    "        svm_obj.fit(training_data,training_labels.ravel())\n",
    "        svmResults = svm_obj.predict(validation_data)\n",
    "        # From the SciKit learn library this method takes in the ground truth labels of the test data and the predicted labels then returns the fraction of correctly classified samples\n",
    "        validation_results.append(metrics.accuracy_score(validation_labels,svmResults))\n",
    "    regParamResults[c] = validation_results\n",
    "\n",
    "# Finding best regularization parameter from tuning on validation sets\n",
    "validationAvgs = {}\n",
    "for regs, val_sets in regParamResults.items():\n",
    "    print('The regularization parameter (C) being used is {}'.format(regs))\n",
    "    avgCorrect = sum(val_sets)/10\n",
    "    print('The average # of correct classifications is {}\\n'.format(avgCorrect))\n",
    "    validationAvgs[regs] = avgCorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From these results we can see that C = 0.1 gives us the best results based on our validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "K-Fold cross validation\n",
    "\"\"\"\n",
    "SVM_k_FoldResults = []\n",
    "kFolds = 10\n",
    "for k_iterCV in range(2,12):\n",
    "    number_of_total_samples = data.shape[0]\n",
    "\n",
    "    fold_size = int(number_of_total_samples/kFolds)\n",
    "\n",
    "    #Splits up the data ito training and testing data for each iteration of K-fold CV\n",
    "    if k_iterCV != 11:\n",
    "        training_data = np.concatenate((data[0:(k_iterCV-2)*fold_size],data[k_iterCV*fold_size:number_of_total_samples]),axis=0)\n",
    "        training_labels = np.concatenate((labels[0:(k_iterCV-2)*fold_size],labels[k_iterCV*fold_size:number_of_total_samples]),axis=0)\n",
    "    else:\n",
    "        training_data = data[fold_size:number_of_total_samples-fold_size]\n",
    "        training_labels = labels[fold_size:number_of_total_samples-fold_size]\n",
    "\n",
    "\n",
    "    testing_data = data[(k_iterCV-2)*fold_size:(k_iterCV-1)*fold_size]\n",
    "    test_labels = labels[(k_iterCV-2)*fold_size:(k_iterCV-1)*fold_size]\n",
    "\n",
    "    if k_iterCV != 11:\n",
    "        validation_data = data[(k_iterCV-1)*fold_size:k_iterCV*fold_size]\n",
    "        validation_labels = labels[(k_iterCV-1)*fold_size:k_iterCV*fold_size]\n",
    "    else:\n",
    "        validation_data = data[0:fold_size]\n",
    "        validation_labels = labels[0:fold_size]\n",
    "    \n",
    "    # fit_intercept set to false because the data was already centered from PCA\n",
    "    # dual set to false to get rid of ConvergenceWarnings\n",
    "    svm_obj = sklSVM.LinearSVC(C = 0.1, dual = False)\n",
    "\n",
    "    #ravel converts narray to (n,) shape which SVM uses\n",
    "    svm_obj.fit(training_data,training_labels.ravel())\n",
    "    svmResults = svm_obj.predict(testing_data)\n",
    "    # From the SciKit learn library this method takes in the ground truth labels of the test data and the predicted labels then returns the fraction of correctly classified samples\n",
    "    SVM_k_FoldResults.append(metrics.accuracy_score(validation_labels,svmResults))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "avgResults = sum(SVM_k_FoldResults)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6388"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avgResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After running the K-Fold cross validation with SVM we find that the best regularization parameter is C = 0.1 and our model has accuracy of about 63.88% on average"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic Discriminant Analysis (by Justin May)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QDA Implementation\n",
    "With this QDA implementation I chose to go with sklearn. Sklearn's behind the scenes optimizations are simply 100x better than mine. Literally, it took around 43 minutes on average to train on each k with n=5000 and p=1719. I probably spent around 14+ hours playing around with optimizing my code and finally just tried sklearn's qda implentation. It finished in less than 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on k=1\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 1\n",
      "Training on k=2\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 2\n",
      "Training on k=3\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 3\n",
      "Training on k=4\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 4\n",
      "Training on k=5\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 5\n",
      "Training on k=6\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 6\n",
      "Training on k=7\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 7\n",
      "Training on k=8\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 8\n",
      "Training on k=9\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 9\n",
      "Training on k=10\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 10\n",
      "[1.0, 0.988, 0.988, 0.994, 0.976, 0.99, 0.98, 0.976, 0.958, 0.962]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=UserWarning)\n",
    "\"\"\"\n",
    "Pull pca data from saved\n",
    "\"\"\"\n",
    "data = np.load('twitterDataReduced.npy')\n",
    "labels = labels[:5000]\n",
    "number_of_parameters = len(data[0]) - 1 #5000\n",
    "number_of_total_samples = len(data)\n",
    "number_of_labels = 2\n",
    "\"\"\"\n",
    "K-Fold cross validation \n",
    "\"\"\"     \n",
    "final_estimates = []\n",
    "k = 10\n",
    "for i in range(1,11):\n",
    "    training_data = np.concatenate((data[0:(i-1)*int(number_of_total_samples/k)],data[i*int(number_of_total_samples/k):number_of_total_samples]),axis=0)\n",
    "    training_labels = np.concatenate((labels[0:(i-1)*int(number_of_total_samples/k)],labels[i*int(number_of_total_samples/k):number_of_total_samples]),axis=0)\n",
    "    testing_data = data[(i-1)*int(number_of_total_samples/k):i*int(number_of_total_samples/k)]\n",
    "    number_of_training_samples = len(training_data)\n",
    "    clf = QDA()\n",
    "    print(\"Training on k=\"+str(i))\n",
    "    clf.fit(training_data, training_labels)\n",
    "    print(\"\")\n",
    "    correct = 0\n",
    "    for index, image in enumerate(testing_data):\n",
    "        if index % 1 == 0:\n",
    "            progress(index+1,int(star/10),suffix=\"testing k= \"+str(i))\n",
    "        if int(testing_data[index][-1]) == int(clf.predict([image])[0]):\n",
    "            correct += 1\n",
    "    sys.stdout.flush()\n",
    "    print(\"\")\n",
    "    final_estimates.append(correct/int(star/10))\n",
    "print(final_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9812"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_estimatesAvg = sum([1.0, 0.988, 0.988, 0.994, 0.976, 0.99, 0.98, 0.976, 0.958, 0.962])/10\n",
    "final_estimatesAvg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression (by: Jonathan Hong)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_reducedTweetsData = np.load('twitterDataReduced.npy')\n",
    "kf = KFold(n_splits=10, random_state=None, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Iteration  1 : \n",
      "\n",
      "\tTraining set: [ 500 ... 4999 ] \tTest set: [ 0 ... 499 ]\n",
      "Score:  0.732\n",
      "Training Iteration  2 : \n",
      "\n",
      "\tTraining set: [ 0 ... 4999 ] \tTest set: [ 500 ... 999 ]\n",
      "Score:  0.706\n",
      "Training Iteration  3 : \n",
      "\n",
      "\tTraining set: [ 0 ... 4999 ] \tTest set: [ 1000 ... 1499 ]\n",
      "Score:  0.766\n",
      "Training Iteration  4 : \n",
      "\n",
      "\tTraining set: [ 0 ... 4999 ] \tTest set: [ 1500 ... 1999 ]\n",
      "Score:  0.8\n",
      "Training Iteration  5 : \n",
      "\n",
      "\tTraining set: [ 0 ... 4999 ] \tTest set: [ 2000 ... 2499 ]\n",
      "Score:  0.744\n",
      "Training Iteration  6 : \n",
      "\n",
      "\tTraining set: [ 0 ... 4999 ] \tTest set: [ 2500 ... 2999 ]\n",
      "Score:  0.816\n",
      "Training Iteration  7 : \n",
      "\n",
      "\tTraining set: [ 0 ... 4999 ] \tTest set: [ 3000 ... 3499 ]\n",
      "Score:  0.82\n",
      "Training Iteration  8 : \n",
      "\n",
      "\tTraining set: [ 0 ... 4999 ] \tTest set: [ 3500 ... 3999 ]\n",
      "Score:  0.766\n",
      "Training Iteration  9 : \n",
      "\n",
      "\tTraining set: [ 0 ... 4999 ] \tTest set: [ 4000 ... 4499 ]\n",
      "Score:  0.738\n",
      "Training Iteration  10 : \n",
      "\n",
      "\tTraining set: [ 0 ... 4499 ] \tTest set: [ 4500 ... 4999 ]\n",
      "Score:  0.742\n",
      "Accuracies: [0.732, 0.706, 0.766, 0.8, 0.744, 0.816, 0.82, 0.766, 0.738, 0.742]\n"
     ]
    }
   ],
   "source": [
    "# initialize\n",
    "accuracy = []\n",
    "models = []\n",
    "count = 1\n",
    "\n",
    "\n",
    "# split into training and validation index\n",
    "for train_index, test_index in kf.split(dim_reducedTweetsData):\n",
    "    print(\"Training Iteration \", count, \": \\n\")\n",
    "    print(\"\\tTraining set: [\", train_index[0], '...', train_index[-1], \"] \\tTest set: [\", test_index[0], '...', test_index[-1], \"]\")\n",
    "    \n",
    "    # split into training and validation dataset\n",
    "    X_train, X_test = dim_reducedTweetsData[train_index], dim_reducedTweetsData[test_index]\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "    \n",
    "    # Use Logistic Regression Model - Binary Case\n",
    "    model = LogisticRegression(solver='liblinear', max_iter = 400)\n",
    "    \n",
    "    # fit training data to model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # report the score using the test dataset\n",
    "    score = model.score(X_test, y_test)\n",
    "    \n",
    "    # append the results for later\n",
    "    accuracy.append(score)\n",
    "    models.append(model)\n",
    "    print(\"Score: \", score)\n",
    "    count +=1\n",
    "# results    \n",
    "print('Accuracies:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.763"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_estimatesAvg = sum([0.732, 0.706, 0.766, 0.8, 0.744, 0.816, 0.82, 0.766, 0.738, 0.742])/10\n",
    "final_estimatesAvg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best iteration at K = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Positive  Negative\n",
      " Positive        45        51\n",
      " Negative        39       365\n"
     ]
    }
   ],
   "source": [
    "indexes = np.arange(5000)\n",
    "test_index = np.arange(3000,3500)\n",
    "model = models[6]\n",
    "predictions = model.predict(dim_reducedTweetsData[test_index])\n",
    "confusion = confusion_matrix(labels[test_index], predictions, labels=['1','0'])\n",
    "sentiment_labels = ['Positive', 'Negative']\n",
    "confusion = pd.DataFrame(confusion)\n",
    "confusion.insert(0, \"\", sentiment_labels, True)\n",
    "confusion.columns = [\"\", 'Positive', 'Negative']\n",
    "print(confusion.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### QDA\n",
    "On average after K-Fold cross validation we get accuracy of about 98.12%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression\n",
    "Results after K-fold: 76.3%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines\n",
    "Results after K-fold: 63.88%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best model for this dataset was also QDA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  (3676, 561)\n",
      "labels:  (3676,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Opening File, creating nparray \n",
    "Data is already 0 mean 1 variance \n",
    "\"\"\"\n",
    "features = open('human_activity_features_train_data.txt','r')\n",
    "human_activity_data = []\n",
    "for feature in features:\n",
    "    feature = np.array([float(w) for w in features.readline().split(\" \") if len(w) > 0])\n",
    "    human_activity_data.append(feature)\n",
    "features.close()\n",
    "human_activity_data = np.array(human_activity_data)\n",
    "print(\"features: \",np.shape(human_activity_data))\n",
    "\n",
    "labels = open('human_activity_labels_train_data.txt','r')\n",
    "human_activity_labels = []\n",
    "for label in labels:\n",
    "    human_activity_labels.append(int(label))\n",
    "labels.close()\n",
    "human_activity_labels = human_activity_labels[:3676]\n",
    "print(\"labels: \",np.shape(human_activity_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster as skCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a StandardScaler object to 0 mean the data matrix but preserve the variance\n",
    "stand_scaler = sklpp.StandardScaler(with_mean = True, with_std = False)\n",
    "# Fits the data matrix to the StandardScaler object defined ^\n",
    "centered_HARData = stand_scaler.fit_transform(human_activity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a PCA object that reduces the dimensions of our data matrix keeping 90% of the variance\n",
    "pca_obj = skldecomp.PCA(n_components = 0.95, svd_solver = 'auto')\n",
    "dim_reducedHARData = pca_obj.fit_transform(centered_HARData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3676, 67)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim_reducedHARData.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('human_activity_data.npy', dim_reducedHARData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K- Means Clustering (by Joseph Shenouda)\n",
    "I implemented K-means clustering using the SciKit Learn library. The **KMeans** function takes in the following important parameters:\n",
    "- n_cluster - The number of clusters to make from the given data\n",
    "- init - The initialization method used for finding the initial centroid in the K-means algorithm, I left this as SciKit's default of k-means++ instead of random for optimization\n",
    "- n_init - The number of times k-means is rerun with different seeds this is import because the more runs we do the better chances we have of finding the global minimum as opposed to the local minima\n",
    "- max_iter - max number of iterations done\n",
    "- tol - Tolerance level used to declare convergence and stop the algo, Left this as default\n",
    "\n",
    "The rest of the algorithm works like the method discussed in class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0 best predicted label is  5 with  96.124 % with cluster size 1290  account for  35.092 % of the data\n",
      "Cluster  1 best predicted label is  2 with  43.047 % with cluster size 1431  account for  38.928 % of the data\n",
      "Cluster  2 best predicted label is  4 with  47.768 % with cluster size 896  account for  24.374 % of the data\n",
      "Cluster  3 best predicted label is  4 with  50.588 % with cluster size 1870  account for  50.871 % of the data\n",
      "Cluster  4 best predicted label is  2 with  64.628 % with cluster size 376  account for  10.229 % of the data\n",
      "Cluster  5 best predicted label is  1 with  54.197 % with cluster size 1489  account for  40.506 % of the data\n"
     ]
    }
   ],
   "source": [
    "kmeans_obj = skCluster.KMeans(n_clusters = 6,init = 'random', n_init = 10)\n",
    "kmeansCluster = kmeans_obj.fit(dim_reducedHARData)\n",
    "\n",
    "predicted_clusters = kmeansCluster.labels_\n",
    "predicted_results = [[0 for j in range(6)] for i in range(6)]\n",
    "\n",
    "for index,cluster in enumerate(predicted_clusters):\n",
    "    predicted_results[cluster][human_activity_labels[index]-1] += 1   \n",
    "for i in range(6):\n",
    "    Sum = sum(predicted_results[i])\n",
    "    predicted_results[i] = [(x/Sum)*100 for x in predicted_results[i]]\n",
    "    print(\"Cluster \",i,\"best predicted label is \",predicted_results[i].index(max(predicted_results[i])),\"with \",'%.3f'%(max(predicted_results[i])),\"% with cluster size\", Sum,\" account for \",'%.3f'%(Sum/3676*100), \"% of the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixtures (by Jonathan Hong)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models\n",
    "\n",
    "GMM is similar to the K means clustering algorithm, but estimates using a mixture of multiple Gaussian distributions and takes into account the means and covariances. K means puts a hypersphere around the cluster centers, but this isn't the best clustering technique if your data is a different shape or if any boundaries are overlapping. So GMM can estimate ellipsoidal shapes using the mean and covariances. It uses the Expectation-Maximization Algorithm, which to iteratively estimate the Maximum Likelihood probabilities of our data belonging to each cluster. This means we have a soft clustering classification because we are given broken down probabilties of a sample for each cluster. These resulting probabilities, also known as responsibilities, tell us how likely our points were estimated to be a part of each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_activity_data = np.load('human_activity_data.npy')\n",
    "gm_model = GaussianMixture(n_components = 6, random_state = 7)\n",
    "gm_model.fit(human_activity_data)\n",
    "pred = gm_model.predict(human_activity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Probabilities: Samples(row) Clusters(columns)\n",
      "[[0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 1. 0.]\n",
      " [1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "probs = gm_model.predict_proba(human_activity_data)\n",
    "print('Predicted Probabilities: Samples(row) Clusters(columns)')\n",
    "print(probs[:20].round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0 best predicted label is  5 with  27.594 % with cluster size 424  account for  11.534 % of the data\n",
      "Cluster  1 best predicted label is  3 with  23.973 % with cluster size 730  account for  19.859 % of the data\n",
      "Cluster  2 best predicted label is  4 with  26.612 % with cluster size 605  account for  16.458 % of the data\n",
      "Cluster  3 best predicted label is  0 with  23.109 % with cluster size 727  account for  19.777 % of the data\n",
      "Cluster  4 best predicted label is  5 with  24.504 % with cluster size 1008  account for  27.421 % of the data\n",
      "Cluster  5 best predicted label is  2 with  28.022 % with cluster size 182  account for  4.951 % of the data\n"
     ]
    }
   ],
   "source": [
    "predicted_clusters = pred\n",
    "predicted_results = [[0 for j in range(6)] for i in range(6)]\n",
    "for index,cluster in enumerate(predicted_clusters):\n",
    "    predicted_results[cluster][human_activity_labels[index]-1] += 1   \n",
    "for i in range(6):\n",
    "    Sum = sum(predicted_results[i])\n",
    "    predicted_results[i] = [(x/Sum)*100 for x in predicted_results[i]]\n",
    "    print(\"Cluster \",i,\"best predicted label is \",predicted_results[i].index(max(predicted_results[i])),\"with \",'%.3f'%(max(predicted_results[i])),\"% with cluster size\", Sum,\" account for \",'%.3f'%(Sum/3676*100), \"% of the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical/Agglomerative Clustering (by: Justin May)\n",
    "\n",
    "This type of clustering breaks all data points down into centroids and groups them one by one until it reaches the specified number of clusters, k. The linkage policy determines grouping, which are:\n",
    "\n",
    "- simple: closest distance between clusters\n",
    "- complete: farthest distance between clusters\n",
    "- average: average distance between clusters\n",
    "- ward: sum of squared differences Our implementation is using euclidean distance\n",
    "\n",
    "Hierarchical/Agglomerative is deterministic and⁠—as compared to k-means⁠—is slow. Complete, Average, and Ward linkage policies yield a $n^{3}$ runtime. Simple linkage yields $n^{2}$ runtime with clever optimizations, which is why we are using sklearn.\n",
    "\n",
    "We are using k=6 because of our a-priori knoweldge that there are 6 groups:\n",
    "\n",
    "1. WALKING,\n",
    "2. WALKING_UPSTAIRS,\n",
    "3. WALKING_DOWNSTAIRS,\n",
    "4. SITTING,\n",
    "5. STANDING,\n",
    "6. LAYING;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0 best predicted label is  4 with  18.801 % with cluster size 3670  account for  99.837 % of the data\n",
      "Cluster  1 best predicted label is  5 with  100.000 % with cluster size 1  account for  0.027 % of the data\n",
      "Cluster  2 best predicted label is  3 with  50.000 % with cluster size 2  account for  0.054 % of the data\n",
      "Cluster  3 best predicted label is  5 with  100.000 % with cluster size 1  account for  0.027 % of the data\n",
      "Cluster  4 best predicted label is  5 with  100.000 % with cluster size 1  account for  0.027 % of the data\n",
      "Cluster  5 best predicted label is  5 with  100.000 % with cluster size 1  account for  0.027 % of the data\n"
     ]
    }
   ],
   "source": [
    "human_activity_data = np.load('human_activity_data.npy')\n",
    "clustering = AgglomerativeClustering(n_clusters = 6, linkage='single').fit(human_activity_data)\n",
    "predicted_clusters = clustering.labels_\n",
    "predicted_results = [[0 for j in range(6)] for i in range(6)]\n",
    "for index,cluster in enumerate(predicted_clusters):\n",
    "    predicted_results[cluster][human_activity_labels[index]-1] += 1   \n",
    "for i in range(6):\n",
    "    Sum = sum(predicted_results[i])\n",
    "    predicted_results[i] = [(x/Sum)*100 for x in predicted_results[i]]\n",
    "    print(\"Cluster \",i,\"best predicted label is \",predicted_results[i].index(max(predicted_results[i])),\"with \",'%.3f'%(max(predicted_results[i])),\"% with cluster size\", Sum,\" account for \",'%.3f'%(Sum/3676*100), \"% of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0 best predicted label is  2 with  28.571 % with cluster size 63  account for  1.714 % of the data\n",
      "Cluster  1 best predicted label is  0 with  27.500 % with cluster size 360  account for  9.793 % of the data\n",
      "Cluster  2 best predicted label is  5 with  23.553 % with cluster size 2021  account for  54.978 % of the data\n",
      "Cluster  3 best predicted label is  5 with  100.000 % with cluster size 9  account for  0.245 % of the data\n",
      "Cluster  4 best predicted label is  3 with  35.294 % with cluster size 17  account for  0.462 % of the data\n",
      "Cluster  5 best predicted label is  3 with  23.466 % with cluster size 1206  account for  32.807 % of the data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "human_activity_data = np.load('human_activity_data.npy')\n",
    "clustering = AgglomerativeClustering(n_clusters = 6, linkage='complete').fit(human_activity_data)\n",
    "predicted_clusters = clustering.labels_\n",
    "predicted_results = [[0 for j in range(6)] for i in range(6)]\n",
    "for index,cluster in enumerate(predicted_clusters):\n",
    "    predicted_results[cluster][human_activity_labels[index]-1] += 1   \n",
    "for i in range(6):\n",
    "    Sum = sum(predicted_results[i])\n",
    "    predicted_results[i] = [(x/Sum)*100 for x in predicted_results[i]]\n",
    "    print(\"Cluster \",i,\"best predicted label is \",predicted_results[i].index(max(predicted_results[i])),\"with \",'%.3f'%(max(predicted_results[i])),\"% with cluster size\", Sum,\" account for \",'%.3f'%(Sum/3676*100), \"% of the data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0 best predicted label is  5 with  100.000 % with cluster size 2  account for  0.054 % of the data\n",
      "Cluster  1 best predicted label is  0 with  41.463 % with cluster size 41  account for  1.115 % of the data\n",
      "Cluster  2 best predicted label is  3 with  22.640 % with cluster size 1568  account for  42.655 % of the data\n",
      "Cluster  3 best predicted label is  2 with  37.037 % with cluster size 27  account for  0.734 % of the data\n",
      "Cluster  4 best predicted label is  5 with  38.462 % with cluster size 13  account for  0.354 % of the data\n",
      "Cluster  5 best predicted label is  5 with  23.506 % with cluster size 2025  account for  55.087 % of the data\n"
     ]
    }
   ],
   "source": [
    "human_activity_data = np.load('human_activity_data.npy')\n",
    "clustering = AgglomerativeClustering(n_clusters = 6, linkage='average').fit(human_activity_data)\n",
    "predicted_clusters = clustering.labels_\n",
    "predicted_results = [[0 for j in range(6)] for i in range(6)]\n",
    "for index,cluster in enumerate(predicted_clusters):\n",
    "    predicted_results[cluster][human_activity_labels[index]-1] += 1   \n",
    "for i in range(6):\n",
    "    Sum = sum(predicted_results[i])\n",
    "    predicted_results[i] = [(x/Sum)*100 for x in predicted_results[i]]\n",
    "    print(\"Cluster \",i,\"best predicted label is \",predicted_results[i].index(max(predicted_results[i])),\"with \",'%.3f'%(max(predicted_results[i])),\"% with cluster size\", Sum,\" account for \",'%.3f'%(Sum/3676*100), \"% of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0 best predicted label is  5 with  26.484 % with cluster size 657  account for  17.873 % of the data\n",
      "Cluster  1 best predicted label is  0 with  25.472 % with cluster size 742  account for  20.185 % of the data\n",
      "Cluster  2 best predicted label is  3 with  24.784 % with cluster size 811  account for  22.062 % of the data\n",
      "Cluster  3 best predicted label is  0 with  26.875 % with cluster size 640  account for  17.410 % of the data\n",
      "Cluster  4 best predicted label is  5 with  25.371 % with cluster size 741  account for  20.158 % of the data\n",
      "Cluster  5 best predicted label is  2 with  23.529 % with cluster size 85  account for  2.312 % of the data\n"
     ]
    }
   ],
   "source": [
    "human_activity_data = np.load('human_activity_data.npy')\n",
    "clustering = AgglomerativeClustering(n_clusters = 6, linkage='ward').fit(human_activity_data)\n",
    "predicted_clusters = clustering.labels_\n",
    "predicted_results = [[0 for j in range(6)] for i in range(6)]\n",
    "for index,cluster in enumerate(predicted_clusters):\n",
    "    predicted_results[cluster][human_activity_labels[index]-1] += 1   \n",
    "for i in range(6):\n",
    "    Sum = sum(predicted_results[i])\n",
    "    predicted_results[i] = [(x/Sum)*100 for x in predicted_results[i]]\n",
    "    print(\"Cluster \",i,\"best predicted label is \",predicted_results[i].index(max(predicted_results[i])),\"with \",'%.3f'%(max(predicted_results[i])),\"% with cluster size\", Sum,\" account for \",'%.3f'%(Sum/3676*100), \"% of the data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
