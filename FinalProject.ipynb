{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Final Project\n",
    "\n",
    "Justin May, Joe Shenouda, Jonathan Hong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import Statements\n",
    "\"\"\"\n",
    "# Python Specific \n",
    "import pickle \n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Data Science \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing as sklpp\n",
    "from sklearn import decomposition as skldecomp\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import csv\n",
    "import warnings\n",
    "import re # for regular expressions\n",
    "\n",
    "# NLP Libraries -- for twitter data preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # A set of the most common stop words \n",
    "from nltk.stem import WordNetLemmatizer # A function to stem words \n",
    "from nltk.tokenize import word_tokenize # An auto-tokenizer \n",
    "# ---- RUN THESE IF RUNNING FOR THE FIRST TIME ----\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# Options\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper Function to See Progress \n",
    "\"\"\"\n",
    "def progress(count, total, suffix=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', suffix))\n",
    "    sys.stdout.flush()  # As suggested by Rom Ruben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting file images/data_batch_1...\n",
      "Finished Extracting Features\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extracting data into a file\n",
    "Data are all in the same domain, no need to normalize\n",
    "\"\"\"\n",
    "data = None\n",
    "labels = []\n",
    "\n",
    "file = \"images/data_batch_1\"\n",
    "with open(file, 'rb') as fo:\n",
    "    print(\"extracting file \"+file+\"...\")\n",
    "    dict = pickle.load(fo, encoding='bytes')\n",
    "    temp_data = dict[b'data']\n",
    "    try:\n",
    "        data = np.concatenate((data, temp_data), axis=0)\n",
    "    except:\n",
    "        data = temp_data\n",
    "    labels = labels + dict[b'labels']\n",
    "labels = np.array(labels)\n",
    "labels = labels.reshape(-1,1)\n",
    "print(\"Finished Extracting Features\")\n",
    "print(np.shape(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Learning For Images Dataset\n",
    "For preprocessing we centered our dataset to 0 mean and then for feature learning we apply PCA to reduce our dimensions from 3072 to a smaller number of features than contains 95% of the variance of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a StandardScaler object to 0 mean the data matrix but preserve the variance\n",
    "stand_scaler = sklpp.StandardScaler(with_mean = True, with_std = False)\n",
    "# Fits the data matrix to the StandardScaler object defined ^\n",
    "centered_ImageData = stand_scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a PCA object that reduces the dimensions of our data matrix keeping 95% of the variance\n",
    "pca_obj = skldecomp.PCA(n_components = 0.95, svd_solver = 'auto')\n",
    "dim_reducedImageData = pca_obj.fit_transform(centered_ImageData)\n",
    "np.save('dim_reducedImageData.npy', dim_reducedImageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been reduced to 209 features after PCA\n"
     ]
    }
   ],
   "source": [
    "print('Data has been reduced to {} features after PCA'.format(dim_reducedImageData.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Model 1) Quadratic Discriminant Analysis: Justin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemeneted the algorithim we learned in class: \n",
    " - first learn the parameters (expected average, prior probabilities, covariance matricies) \n",
    " - define the discriminant functions to find the best labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...training k= 1\n",
      "0.466========================================================] 100.0% ...testing k= 1\n",
      "[============================================================] 100.0% ...training k= 2\n",
      "0.498========================================================] 100.0% ...testing k= 2\n",
      "[============================================================] 100.0% ...training k= 3\n",
      "0.479========================================================] 100.0% ...testing k= 3\n",
      "[============================================================] 100.0% ...training k= 4\n",
      "0.473========================================================] 100.0% ...testing k= 4\n",
      "[============================================================] 100.0% ...training k= 5\n",
      "0.464========================================================] 100.0% ...testing k= 5\n",
      "[============================================================] 100.0% ...training k= 6\n",
      "0.49=========================================================] 100.0% ...testing k= 6\n",
      "[============================================================] 100.0% ...training k= 7\n",
      "0.441========================================================] 100.0% ...testing k= 7\n",
      "[============================================================] 100.0% ...training k= 8\n",
      "0.469========================================================] 100.0% ...testing k= 8\n",
      "[============================================================] 100.0% ...training k= 9\n",
      "0.482========================================================] 100.0% ...testing k= 9\n",
      "[============================================================] 100.0% ...training k= 10\n",
      "0.473========================================================] 100.0% ...testing k= 10\n",
      "\n",
      "[0.466, 0.498, 0.479, 0.473, 0.464, 0.49, 0.441, 0.469, 0.482, 0.473]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pull pca data from saved\n",
    "\"\"\"\n",
    "data = np.load('dim_reducedImageData.npy')\n",
    "data = np.concatenate((data,labels),axis=1)\n",
    "number_of_parameters = len(data[0]) - 1 #209\n",
    "number_of_total_samples = len(data)\n",
    "\"\"\"\n",
    "K-Fold cross validation \n",
    "\"\"\"     \n",
    "final_estimates = []\n",
    "k = 10\n",
    "for i in range(1,k+1):\n",
    "    training_data = np.concatenate((data[0:(i-1)*int(number_of_total_samples/k)],data[i*int(number_of_total_samples/k):number_of_total_samples]),axis=0)\n",
    "    testing_data = data[(i-1)*int(number_of_total_samples/k):i*int(number_of_total_samples/k)]\n",
    "    number_of_training_samples = len(training_data)\n",
    "    \"\"\"\n",
    "    Define Parameters \n",
    "    \"\"\"\n",
    "    mu = [[0 for j in range(number_of_parameters)] for i in range(10)] # initializing means \n",
    "    sigmas = [[[0 for k in range(number_of_parameters)] for j in range(number_of_parameters)] for i in range(10)] # initializing variances\n",
    "    occurences = [0 for i in range(10)]\n",
    "    pi = [0 for i in range(10)]\n",
    "        \n",
    "    \"\"\"\n",
    "    Learn mu\n",
    "    \"\"\"\n",
    "    # Find Sums and Occurences\n",
    "    for image in training_data:\n",
    "        label = int(image[-1])\n",
    "        occurences[label] += 1\n",
    "        mu[label] = np.add(mu[label],image[:-1])\n",
    "    # Calculate Averages and Prior Probabilities \n",
    "    for label,sums in enumerate(mu):\n",
    "        mu[label] = np.multiply(1/(occurences[label] - 1), sums).reshape(-1,1) #unbaised estimator\n",
    "        pi[label] = occurences[label] / number_of_total_samples\n",
    "    \"\"\"\n",
    "    Learn sigma^2\n",
    "    \"\"\"\n",
    "    time = 0\n",
    "    for image in training_data:\n",
    "        time += 1\n",
    "        if time%10 == 0:\n",
    "            progress(time,number_of_training_samples,suffix=\"training k= \"+str(i))\n",
    "        label = int(image[-1])\n",
    "        image = image[:-1].reshape(-1,1)\n",
    "        difference = np.subtract(image,mu[label])\n",
    "        sigma = difference.dot(difference.T)\n",
    "        sigmas[label] = np.add(sigmas[label],sigma)\n",
    "    for label,sigma in enumerate(sigmas):\n",
    "        sigmas[label] = np.multiply(1/(occurences[label]-1),sigma)\n",
    "    sys.stdout.flush()\n",
    "    \"\"\"\n",
    "    Method to find the best discriminant score\n",
    "    \"\"\"\n",
    "    def estimateBestLabel(image):\n",
    "        scores = [0 for _ in range(10)]\n",
    "        # find score for each label \n",
    "\n",
    "        for label in range(10):\n",
    "            inverted_variance = np.linalg.inv(sigmas[label])\n",
    "            first_term = -0.5*image.T.dot(inverted_variance).dot(image)\n",
    "            second_term = image.T.dot(inverted_variance).dot(mu[label])\n",
    "            third_term = -0.5*mu[label].T.dot(inverted_variance).dot(mu[label])\n",
    "            (sign, logdet) = np.linalg.slogdet(sigmas[label])\n",
    "            fourth_term = -0.5*sign*logdet\n",
    "            fifth_term = math.log(pi[label])\n",
    "            score = first_term + second_term + third_term + fourth_term + fifth_term\n",
    "            scores[label] = score[0][0]\n",
    "        return(scores.index(max(scores)))\n",
    "    correct = 0\n",
    "    print(\"\")\n",
    "    for index, image in enumerate(testing_data):\n",
    "        if index % 9 == 0:\n",
    "            progress(index+1,1000,suffix=\"testing k= \"+str(i))\n",
    "        image = image[:-1].reshape(-1,1)\n",
    "        if testing_data[index][-1] == estimateBestLabel(image):\n",
    "            correct += 1\n",
    "    sys.stdout.flush()\n",
    "    print(correct/1000)\n",
    "    final_estimates.append(correct/1000)\n",
    "print(\"\")\n",
    "print(final_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for best K (k = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...training k= 6\n",
      "[============================================================] 100.0% ...testing k= 6\r"
     ]
    }
   ],
   "source": [
    "data = np.load('dim_reducedImageData.npy')\n",
    "data = np.concatenate((data,labels),axis=1)\n",
    "number_of_parameters = len(data[0]) - 1 #209\n",
    "number_of_total_samples = len(data)\n",
    "\n",
    "k = 10\n",
    "i = 6\n",
    "training_data = np.concatenate((data[0:(i-1)*int(number_of_total_samples/k)],data[i*int(number_of_total_samples/k):number_of_total_samples]),axis=0)\n",
    "testing_data = data[(i-1)*int(number_of_total_samples/k):i*int(number_of_total_samples/k)]\n",
    "number_of_training_samples = len(training_data)\n",
    "\"\"\"\n",
    "Define Parameters \n",
    "\"\"\"\n",
    "mu = [[0 for j in range(number_of_parameters)] for i in range(10)] # initializing means \n",
    "sigmas = [[[0 for k in range(number_of_parameters)] for j in range(number_of_parameters)] for i in range(10)] # initializing variances\n",
    "occurences = [0 for i in range(10)]\n",
    "pi = [0 for i in range(10)]\n",
    "\n",
    "\"\"\"\n",
    "Learn mu\n",
    "\"\"\"\n",
    "# Find Sums and Occurences\n",
    "for image in training_data:\n",
    "    label = int(image[-1])\n",
    "    occurences[label] += 1\n",
    "    mu[label] = np.add(mu[label],image[:-1])\n",
    "# Calculate Averages and Prior Probabilities \n",
    "for label,sums in enumerate(mu):\n",
    "    mu[label] = np.multiply(1/(occurences[label] - 1), sums).reshape(-1,1) #unbaised estimator\n",
    "    pi[label] = occurences[label] / number_of_total_samples\n",
    "\"\"\"\n",
    "Learn sigma^2\n",
    "\"\"\"\n",
    "time = 0\n",
    "for image in training_data:\n",
    "    time += 1\n",
    "    if time%10 == 0:\n",
    "        progress(time,number_of_training_samples,suffix=\"training k= \"+str(i))\n",
    "    label = int(image[-1])\n",
    "    image = image[:-1].reshape(-1,1)\n",
    "    difference = np.subtract(image,mu[label])\n",
    "    sigma = difference.dot(difference.T)\n",
    "    sigmas[label] = np.add(sigmas[label],sigma)\n",
    "for label,sigma in enumerate(sigmas):\n",
    "    sigmas[label] = np.multiply(1/(occurences[label]-1),sigma)\n",
    "sys.stdout.flush()\n",
    "\"\"\"\n",
    "Method to find the best discriminant score\n",
    "\"\"\"\n",
    "def estimateBestLabel(image):\n",
    "    scores = [0 for _ in range(10)]\n",
    "    # find score for each label \n",
    "\n",
    "    for label in range(10):\n",
    "        inverted_variance = np.linalg.inv(sigmas[label])\n",
    "        first_term = -0.5*image.T.dot(inverted_variance).dot(image)\n",
    "        second_term = image.T.dot(inverted_variance).dot(mu[label])\n",
    "        third_term = -0.5*mu[label].T.dot(inverted_variance).dot(mu[label])\n",
    "        (sign, logdet) = np.linalg.slogdet(sigmas[label])\n",
    "        fourth_term = -0.5*sign*logdet\n",
    "        fifth_term = math.log(pi[label])\n",
    "        score = first_term + second_term + third_term + fourth_term + fifth_term\n",
    "        scores[label] = score[0][0]\n",
    "    return(scores.index(max(scores)))\n",
    "correct = 0\n",
    "print(\"\")\n",
    "estimated = []\n",
    "real = []\n",
    "for index, image in enumerate(testing_data):\n",
    "    if index % 9 == 0:\n",
    "        progress(index+1,1000,suffix=\"testing k= \"+str(i))\n",
    "    image = image[:-1].reshape(-1,1)\n",
    "    real_label = testing_data[index][-1]\n",
    "    estimated_label = estimateBestLabel(image)\n",
    "    real.append(real_label)\n",
    "    estimated.append(estimated_label)\n",
    "sys.stdout.flush()\n",
    "confusion = confusion_matrix(real, estimated,labels=[0,1,2,3,4,5,6,7,8,9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             airplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck\n",
      "   airplane        46          12     2    5     3    2     2      1    22     10\n",
      " automobile         3          67     1    1     1    1     0      1     5     11\n",
      "       bird        11           3    28   10    19    6     7      2     4     10\n",
      "        cat         2           5     9   27     9   14     9      6     1     12\n",
      "       deer         6           7     2    6    47    4     4      8     4      4\n",
      "        dog         0           8     7   14     4   44     2      8     2      6\n",
      "       frog         2           9     8   10     8    5    39      6     2      8\n",
      "      horse         1           3     3    9     2    8     4     69     1     11\n",
      "       ship         9          13     2    3     5    2     4      0    48     14\n",
      "      truck         3          19     1    3     1    3     1      3     6     75\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Print Confusion Matrix \n",
    "\"\"\"\n",
    "image_labels = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship' ,'truck']\n",
    "confusion = pd.DataFrame(confusion)\n",
    "confusion.insert(0, \"\", image_labels, True)\n",
    "confusion.columns = [\"\", 'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship' ,'truck']\n",
    "print(confusion.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Model 2) Logistic Regression: Jon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Model 3) Support Vector Machine: Joe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "\n",
    "### QDA\n",
    "[0.466, 0.498, 0.479, 0.473, 0.464, 0.49, 0.441, 0.469, 0.482, 0.473]\n",
    "\n",
    "### Logistics Regression \n",
    "\n",
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data - Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our twitter data preprocessing step consistions of the following: \n",
    "   1. Tokenizing words the tweets \n",
    "   2. Removing any links, @ mentions, and #tags that don't have semantic meaning outside of twitter\n",
    "   3. Lemmatization: remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma\n",
    "   4. Lower case all words \n",
    "   5. Remove stopwords: a commonly used word (such as “the”, “a”, “an”, “in”) that have little semantic meaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading in Data\n",
    "\"\"\"\n",
    "df_raw_twitter_data = pd.read_csv('train.csv', header=None, encoding = \"ISO-8859-1\").values[1:]\n",
    "df_raw_twitter_data = df_raw_twitter_data[:,1:] # getting rid of index \n",
    "\n",
    "labels = df_raw_twitter_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 2)\n",
      "\n",
      "[============================================================] 99.9% ...running twitter processing\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Preprocessing\n",
    "\"\"\"\n",
    "print(np.shape(df_raw_twitter_data))\n",
    "corpus = []\n",
    "print(\"\")\n",
    "time = 0\n",
    "for index,[sentiment, tweet] in enumerate(df_raw_twitter_data):\n",
    "    time += 1\n",
    "    # Tokenize Words \n",
    "    tokens = tweet.split(\" \")\n",
    "    # Remove Links, @ mentions, # tags\n",
    "    links = ['http', '.com', '#', '@', '&', '~']\n",
    "    tokens = [w for w in tokens if not any(x in w for x in links)]\n",
    "    # Tokenize again \n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    tokens = [regex.sub('', w) for w in tokens]\n",
    "    # Lemmatization \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens ]\n",
    "    # Clean Up\n",
    "    tokens = [w.lower() for w in tokens if len(w) > 0]\n",
    "    # Remove Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('im')\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    corpus.append(\" \".join(tokens))\n",
    "    if time % 100 == 0:\n",
    "        progress(time,99988,suffix=\"running twitter processing\")\n",
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the Words \n",
    "CountVectorizer() is a sklearn library that turns a list of strings into a $n\\mathbb{x}p$ array. Each row corresponds to a sample $x_{i}$ and each column corresponds to a unique word. The value is the occurence of the word in the tweet. We are using a bag-of-words approach, the simplest approach.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished vectorization\n",
      "(5000, 7882)\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "star = 5000\n",
    "##############\n",
    "corpus = np.load('corpus.npy')\n",
    "vectorizer = CountVectorizer()\n",
    "vobj = vectorizer.fit_transform(corpus[:star])\n",
    "vectors = vobj.toarray()\n",
    "print(\"\\nFinished vectorization\")\n",
    "print(np.shape(vectors))\n",
    "np.save('corpus.npy', vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a StandardScaler object to 0 mean the data matrix but preserve the variance\n",
    "stand_scaler = sklpp.StandardScaler(with_mean = True, with_std = False)\n",
    "# Fits the data matrix to the StandardScaler object defined ^\n",
    "centered_ImageData = stand_scaler.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a PCA object that reduces the dimensions of our data matrix keeping 95% of the variance\n",
    "pca_obj = skldecomp.PCA(n_components = 0.90, svd_solver = 'auto')\n",
    "dim_reducedImageData = pca_obj.fit_transform(centered_ImageData)\n",
    "np.save('twitterDataReduced.npy', dim_reducedImageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been reduced to 1719 features after PCA\n"
     ]
    }
   ],
   "source": [
    "print('Data has been reduced to {} features after PCA'.format(dim_reducedImageData.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Model 1) Quadratic Discriminant Analysis: Justin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QDA Implementation ###\n",
    "With this QDA implementation I chose to go with sklearn. Sklearn's behind the scenes optimizations are simply 100x better than mine. Literally, it took around 43 minutes on average to train on each k with n=5000 and p=1719. I probably spent around 14+ hours playing around with optimizing my code and finally just tried sklearn's qda implentation. It finished in less than 5 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on k=1\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 1\n",
      "Training on k=2\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 2\n",
      "Training on k=3\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 3\n",
      "Training on k=4\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 4\n",
      "Training on k=5\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 5\n",
      "Training on k=6\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 6\n",
      "Training on k=7\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 7\n",
      "Training on k=8\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 8\n",
      "Training on k=9\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 9\n",
      "Training on k=10\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 10\n",
      "[1.0, 0.988, 0.988, 0.994, 0.976, 0.99, 0.98, 0.976, 0.958, 0.962]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "\"\"\"\n",
    "Pull pca data from saved\n",
    "\"\"\"\n",
    "data = np.load('twitterDataReduced.npy')\n",
    "labels = labels[:5000]\n",
    "number_of_parameters = len(data[0]) - 1 #5000\n",
    "number_of_total_samples = len(data)\n",
    "number_of_labels = 2\n",
    "\"\"\"\n",
    "K-Fold cross validation \n",
    "\"\"\"     \n",
    "final_estimates = []\n",
    "k = 10\n",
    "for i in range(1,11):\n",
    "    training_data = np.concatenate((data[0:(i-1)*int(number_of_total_samples/k)],data[i*int(number_of_total_samples/k):number_of_total_samples]),axis=0)\n",
    "    training_labels = np.concatenate((labels[0:(i-1)*int(number_of_total_samples/k)],labels[i*int(number_of_total_samples/k):number_of_total_samples]),axis=0)\n",
    "    testing_data = data[(i-1)*int(number_of_total_samples/k):i*int(number_of_total_samples/k)]\n",
    "    number_of_training_samples = len(training_data)\n",
    "    clf = QDA()\n",
    "    print(\"Training on k=\"+str(i))\n",
    "    clf.fit(training_data, training_labels)\n",
    "    print(\"\")\n",
    "    correct = 0\n",
    "    for index, image in enumerate(testing_data):\n",
    "        if index % 1 == 0:\n",
    "            progress(index+1,int(star/10),suffix=\"testing k= \"+str(i))\n",
    "        if int(testing_data[index][-1]) == int(clf.predict([image])[0]):\n",
    "            correct += 1\n",
    "    sys.stdout.flush()\n",
    "    print(\"\")\n",
    "    final_estimates.append(correct/int(star/10))\n",
    "print(final_estimates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Model 2) Logistic Regression: Jon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Model 3) ??? : Joe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  (3676, 561)\n",
      "labels:  (3676,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Opening File, creating nparray \n",
    "Data is already 0 mean 1 variance \n",
    "\"\"\"\n",
    "features = open('human_activity_features_train_data.txt','r')\n",
    "human_activity_data = []\n",
    "for feature in features:\n",
    "    feature = np.array([float(w) for w in features.readline().split(\" \") if len(w) > 0])\n",
    "    human_activity_data.append(feature)\n",
    "features.close()\n",
    "human_activity_data = np.array(human_activity_data)\n",
    "print(\"features: \",np.shape(human_activity_data))\n",
    "\n",
    "labels = open('human_activity_labels_train_data.txt','r')\n",
    "human_activity_labels = []\n",
    "for label in labels:\n",
    "    human_activity_labels.append(int(label))\n",
    "labels.close()\n",
    "human_activity_labels = human_activity_labels[:3676]\n",
    "print(\"labels: \",np.shape(human_activity_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a StandardScaler object to 0 mean the data matrix but preserve the variance\n",
    "stand_scaler = sklpp.StandardScaler(with_mean = True, with_std = False)\n",
    "# Fits the data matrix to the StandardScaler object defined ^\n",
    "centered_ImageData = stand_scaler.fit_transform(human_activity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a PCA object that reduces the dimensions of our data matrix keeping 95% of the variance\n",
    "pca_obj = skldecomp.PCA(n_components = 0.95, svd_solver = 'auto')\n",
    "dim_reducedImageData = pca_obj.fit_transform(centered_ImageData)\n",
    "np.save('human_activity_data.npy', dim_reducedImageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been reduced to 153 features after PCA\n"
     ]
    }
   ],
   "source": [
    "print('Data has been reduced to {} features after PCA'.format(dim_reducedImageData.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Model 1) Hierarchical/Agglomerative Clustering - Justin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of clustering breaks all data points down into centroids and groups them one by one until it reaches the specified number of clusters, k. The linkage policy determines grouping, which are:\n",
    " - simple: closest distance between clusters \n",
    " - complete: farthest distance between clusters \n",
    " - average: average distance between clusters \n",
    " - ward: sum of squared differences\n",
    "Our implementation is using euclidean distance \n",
    "\n",
    "Hierarchical/Agglomerative is deterministic and⁠—as compared to k-means⁠—is slow. Complete, Average, and Ward linkage policies yield a $n^{3}$ runtime. Simple linkage yields $n^{2}$ runtime with clever optimizations, which is why we are using sklearn. \n",
    "\n",
    "We are using k=6 because of our a-priori knoweldge that there are 6 groups:\n",
    " 1. WALKING,\n",
    " 2. WALKING_UPSTAIRS,\n",
    " 3. WALKING_DOWNSTAIRS,\n",
    " 4. SITTING,\n",
    " 5. STANDING,\n",
    " 6. LAYING;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0 best predicted label is  4 with  18.801 % with cluster size 3670  account for  99.837 % of the data\n",
      "Cluster  1 best predicted label is  5 with  100.000 % with cluster size 1  account for  0.027 % of the data\n",
      "Cluster  2 best predicted label is  3 with  50.000 % with cluster size 2  account for  0.054 % of the data\n",
      "Cluster  3 best predicted label is  5 with  100.000 % with cluster size 1  account for  0.027 % of the data\n",
      "Cluster  4 best predicted label is  5 with  100.000 % with cluster size 1  account for  0.027 % of the data\n",
      "Cluster  5 best predicted label is  5 with  100.000 % with cluster size 1  account for  0.027 % of the data\n"
     ]
    }
   ],
   "source": [
    "human_activity_data = np.load('human_activity_data.npy')\n",
    "clustering = AgglomerativeClustering(n_clusters = 6, linkage='single').fit(human_activity_data)\n",
    "predicted_clusters = clustering.labels_\n",
    "predicted_results = [[0 for j in range(6)] for i in range(6)]\n",
    "for index,cluster in enumerate(predicted_clusters):\n",
    "    predicted_results[cluster][human_activity_labels[index]-1] += 1   \n",
    "for i in range(6):\n",
    "    Sum = sum(predicted_results[i])\n",
    "    predicted_results[i] = [(x/Sum)*100 for x in predicted_results[i]]\n",
    "    print(\"Cluster \",i,\"best predicted label is \",predicted_results[i].index(max(predicted_results[i])),\"with \",'%.3f'%(max(predicted_results[i])),\"% with cluster size\", Sum,\" account for \",'%.3f'%(Sum/3676*100), \"% of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0 best predicted label is  2 with  28.571 % with cluster size 63  account for  1.714 % of the data\n",
      "Cluster  1 best predicted label is  0 with  27.500 % with cluster size 360  account for  9.793 % of the data\n",
      "Cluster  2 best predicted label is  5 with  23.553 % with cluster size 2021  account for  54.978 % of the data\n",
      "Cluster  3 best predicted label is  5 with  100.000 % with cluster size 9  account for  0.245 % of the data\n",
      "Cluster  4 best predicted label is  3 with  35.294 % with cluster size 17  account for  0.462 % of the data\n",
      "Cluster  5 best predicted label is  3 with  23.466 % with cluster size 1206  account for  32.807 % of the data\n"
     ]
    }
   ],
   "source": [
    "human_activity_data = np.load('human_activity_data.npy')\n",
    "clustering = AgglomerativeClustering(n_clusters = 6, linkage='complete').fit(human_activity_data)\n",
    "predicted_clusters = clustering.labels_\n",
    "predicted_results = [[0 for j in range(6)] for i in range(6)]\n",
    "for index,cluster in enumerate(predicted_clusters):\n",
    "    predicted_results[cluster][human_activity_labels[index]-1] += 1   \n",
    "for i in range(6):\n",
    "    Sum = sum(predicted_results[i])\n",
    "    predicted_results[i] = [(x/Sum)*100 for x in predicted_results[i]]\n",
    "    print(\"Cluster \",i,\"best predicted label is \",predicted_results[i].index(max(predicted_results[i])),\"with \",'%.3f'%(max(predicted_results[i])),\"% with cluster size\", Sum,\" account for \",'%.3f'%(Sum/3676*100), \"% of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0 best predicted label is  5 with  100.000 % with cluster size 2  account for  0.054 % of the data\n",
      "Cluster  1 best predicted label is  0 with  41.463 % with cluster size 41  account for  1.115 % of the data\n",
      "Cluster  2 best predicted label is  3 with  22.640 % with cluster size 1568  account for  42.655 % of the data\n",
      "Cluster  3 best predicted label is  2 with  37.037 % with cluster size 27  account for  0.734 % of the data\n",
      "Cluster  4 best predicted label is  5 with  38.462 % with cluster size 13  account for  0.354 % of the data\n",
      "Cluster  5 best predicted label is  5 with  23.506 % with cluster size 2025  account for  55.087 % of the data\n"
     ]
    }
   ],
   "source": [
    "human_activity_data = np.load('human_activity_data.npy')\n",
    "clustering = AgglomerativeClustering(n_clusters = 6, linkage='average').fit(human_activity_data)\n",
    "predicted_clusters = clustering.labels_\n",
    "predicted_results = [[0 for j in range(6)] for i in range(6)]\n",
    "for index,cluster in enumerate(predicted_clusters):\n",
    "    predicted_results[cluster][human_activity_labels[index]-1] += 1   \n",
    "for i in range(6):\n",
    "    Sum = sum(predicted_results[i])\n",
    "    predicted_results[i] = [(x/Sum)*100 for x in predicted_results[i]]\n",
    "    print(\"Cluster \",i,\"best predicted label is \",predicted_results[i].index(max(predicted_results[i])),\"with \",'%.3f'%(max(predicted_results[i])),\"% with cluster size\", Sum,\" account for \",'%.3f'%(Sum/3676*100), \"% of the data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster  0 best predicted label is  5 with  26.484 % with cluster size 657  account for  17.873 % of the data\n",
      "Cluster  1 best predicted label is  0 with  25.472 % with cluster size 742  account for  20.185 % of the data\n",
      "Cluster  2 best predicted label is  3 with  24.784 % with cluster size 811  account for  22.062 % of the data\n",
      "Cluster  3 best predicted label is  0 with  26.875 % with cluster size 640  account for  17.410 % of the data\n",
      "Cluster  4 best predicted label is  5 with  25.371 % with cluster size 741  account for  20.158 % of the data\n",
      "Cluster  5 best predicted label is  2 with  23.529 % with cluster size 85  account for  2.312 % of the data\n"
     ]
    }
   ],
   "source": [
    "human_activity_data = np.load('human_activity_data.npy')\n",
    "clustering = AgglomerativeClustering(n_clusters = 6, linkage='ward').fit(human_activity_data)\n",
    "predicted_clusters = clustering.labels_\n",
    "predicted_results = [[0 for j in range(6)] for i in range(6)]\n",
    "for index,cluster in enumerate(predicted_clusters):\n",
    "    predicted_results[cluster][human_activity_labels[index]-1] += 1   \n",
    "for i in range(6):\n",
    "    Sum = sum(predicted_results[i])\n",
    "    predicted_results[i] = [(x/Sum)*100 for x in predicted_results[i]]\n",
    "    print(\"Cluster \",i,\"best predicted label is \",predicted_results[i].index(max(predicted_results[i])),\"with \",'%.3f'%(max(predicted_results[i])),\"% with cluster size\", Sum,\" account for \",'%.3f'%(Sum/3676*100), \"% of the data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Model 2) ?? - Jon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Model 3) K-Nearnest Neighbors (KNN) - Joe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
