{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "\n",
    "def progress(count, total, suffix=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', suffix))\n",
    "    sys.stdout.flush()  # As suggested by Rom Ruben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math\n",
    "from sklearn import preprocessing as sklpp\n",
    "from sklearn import decomposition as skldecomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting file images/data_batch_1...\n",
      "Finished Extracting Features\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extracting data into a file\n",
    "Data are all in the same domain, no need to normalize\n",
    "\"\"\"\n",
    "data = None\n",
    "labels = []\n",
    "\n",
    "file = \"images/data_batch_1\"\n",
    "with open(file, 'rb') as fo:\n",
    "    print(\"extracting file \"+file+\"...\")\n",
    "    dict = pickle.load(fo, encoding='bytes')\n",
    "    temp_data = dict[b'data']\n",
    "    try:\n",
    "        data = np.concatenate((data, temp_data), axis=0)\n",
    "    except:\n",
    "        data = temp_data\n",
    "    labels = labels + dict[b'labels']\n",
    "labels = np.array(labels)\n",
    "labels = labels.reshape(-1,1)\n",
    "print(\"Finished Extracting Features\")\n",
    "print(np.shape(data))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Learning For Images Dataset\n",
    "For preprocessing we centered our dataset to 0 mean and then for feature learning we apply PCA to reduce our dimensions from 3072 to a smaller number of features than contains 95% of the variance of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a StandardScaler object to 0 mean the data matrix but preserve the variance\n",
    "stand_scaler = sklpp.StandardScaler(with_mean = True, with_std = False)\n",
    "# Fits the data matrix to the StandardScaler object defined ^\n",
    "centered_ImageData = stand_scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a PCA object that reduces the dimensions of our data matrix keeping 95% of the variance\n",
    "pca_obj = skldecomp.PCA(n_components = 0.95, svd_solver = 'auto')\n",
    "dim_reducedImageData = pca_obj.fit_transform(centered_ImageData)\n",
    "np.save('dim_reducedImageData.npy', dim_reducedImageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been reduced to 209 features after PCA\n"
     ]
    }
   ],
   "source": [
    "print('Data has been reduced to {} features after PCA'.format(dim_reducedImageData.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QDA: Justin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...training k= 1\n",
      "0.466========================================================] 100.0% ...testing k= 1\n",
      "[============================================================] 100.0% ...training k= 2\n",
      "0.498========================================================] 100.0% ...testing k= 2\n",
      "[============================================================] 100.0% ...training k= 3\n",
      "0.479========================================================] 100.0% ...testing k= 3\n",
      "[============================================================] 100.0% ...training k= 4\n",
      "0.473========================================================] 100.0% ...testing k= 4\n",
      "[============================================================] 100.0% ...training k= 5\n",
      "0.464========================================================] 100.0% ...testing k= 5\n",
      "[============================================================] 100.0% ...training k= 6\n",
      "0.49=========================================================] 100.0% ...testing k= 6\n",
      "[============================================================] 100.0% ...training k= 7\n",
      "0.441========================================================] 100.0% ...testing k= 7\n",
      "[============================================================] 100.0% ...training k= 8\n",
      "0.469========================================================] 100.0% ...testing k= 8\n",
      "[============================================================] 100.0% ...training k= 9\n",
      "0.482========================================================] 100.0% ...testing k= 9\n",
      "[============================================================] 100.0% ...training k= 10\n",
      "0.473========================================================] 100.0% ...testing k= 10\n",
      "\n",
      "[0.466, 0.498, 0.479, 0.473, 0.464, 0.49, 0.441, 0.469, 0.482, 0.473]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pull pca data from saved\n",
    "\"\"\"\n",
    "data = np.load('dim_reducedImageData.npy')\n",
    "data = np.concatenate((data,labels),axis=1)\n",
    "number_of_parameters = len(data[0]) - 1 #209\n",
    "number_of_total_samples = len(data)\n",
    "\"\"\"\n",
    "K-Fold cross validation \n",
    "\"\"\"     \n",
    "final_estimates = []\n",
    "k = 10\n",
    "for i in range(1,k+1):\n",
    "    training_data = np.concatenate((data[0:(i-1)*int(number_of_total_samples/k)],data[i*int(number_of_total_samples/k):number_of_total_samples]),axis=0)\n",
    "    testing_data = data[(i-1)*int(number_of_total_samples/k):i*int(number_of_total_samples/k)]\n",
    "    number_of_training_samples = len(training_data)\n",
    "    \"\"\"\n",
    "    Define Parameters \n",
    "    \"\"\"\n",
    "    mu = [[0 for j in range(number_of_parameters)] for i in range(10)] # initializing means \n",
    "    sigmas = [[[0 for k in range(number_of_parameters)] for j in range(number_of_parameters)] for i in range(10)] # initializing variances\n",
    "    occurences = [0 for i in range(10)]\n",
    "    pi = [0 for i in range(10)]\n",
    "        \n",
    "    \"\"\"\n",
    "    Learn mu\n",
    "    \"\"\"\n",
    "    # Find Sums and Occurences\n",
    "    for image in training_data:\n",
    "        label = int(image[-1])\n",
    "        occurences[label] += 1\n",
    "        mu[label] = np.add(mu[label],image[:-1])\n",
    "    # Calculate Averages and Prior Probabilities \n",
    "    for label,sums in enumerate(mu):\n",
    "        mu[label] = np.multiply(1/(occurences[label] - 1), sums).reshape(-1,1) #unbaised estimator\n",
    "        pi[label] = occurences[label] / number_of_total_samples\n",
    "    \"\"\"\n",
    "    Learn sigma^2\n",
    "    \"\"\"\n",
    "    time = 0\n",
    "    for image in training_data:\n",
    "        time += 1\n",
    "        if time%10 == 0:\n",
    "            progress(time,number_of_training_samples,suffix=\"training k= \"+str(i))\n",
    "        label = int(image[-1])\n",
    "        image = image[:-1].reshape(-1,1)\n",
    "        difference = np.subtract(image,mu[label])\n",
    "        sigma = difference.dot(difference.T)\n",
    "        sigmas[label] = np.add(sigmas[label],sigma)\n",
    "    for label,sigma in enumerate(sigmas):\n",
    "        sigmas[label] = np.multiply(1/(occurences[label]-1),sigma)\n",
    "    sys.stdout.flush()\n",
    "    \"\"\"\n",
    "    Method to find the best discriminant score\n",
    "    \"\"\"\n",
    "    def estimateBestLabel(image):\n",
    "        scores = [0 for _ in range(10)]\n",
    "        # find score for each label \n",
    "\n",
    "        for label in range(10):\n",
    "            inverted_variance = np.linalg.inv(sigmas[label])\n",
    "            first_term = -0.5*image.T.dot(inverted_variance).dot(image)\n",
    "            second_term = image.T.dot(inverted_variance).dot(mu[label])\n",
    "            third_term = -0.5*mu[label].T.dot(inverted_variance).dot(mu[label])\n",
    "            (sign, logdet) = np.linalg.slogdet(sigmas[label])\n",
    "            fourth_term = -0.5*sign*logdet\n",
    "            fifth_term = math.log(pi[label])\n",
    "            score = first_term + second_term + third_term + fourth_term + fifth_term\n",
    "            scores[label] = score[0][0]\n",
    "        return(scores.index(max(scores)))\n",
    "    correct = 0\n",
    "    print(\"\")\n",
    "    for index, image in enumerate(testing_data):\n",
    "        if index % 9 == 0:\n",
    "            progress(index+1,1000,suffix=\"testing k= \"+str(i))\n",
    "        image = image[:-1].reshape(-1,1)\n",
    "        if testing_data[index][-1] == estimateBestLabel(image):\n",
    "            correct += 1\n",
    "    sys.stdout.flush()\n",
    "    print(correct/1000)\n",
    "    final_estimates.append(correct/1000)\n",
    "print(\"\")\n",
    "print(final_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results \n",
    "% Correct for each k\n",
    "\n",
    "[0.466, 0.498, 0.479, 0.473, 0.464, 0.49, 0.441, 0.469, 0.482, 0.473]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import time\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading in Data\n",
    "\"\"\"\n",
    "df_raw_twitter_data = pd.read_csv('train.csv', header=None, encoding = \"ISO-8859-1\").values[1:]\n",
    "df_raw_twitter_data = df_raw_twitter_data[:,1:] # getting rid of index \n",
    "\n",
    "labels = df_raw_twitter_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 2)\n",
      "\n",
      "[============================================================] 99.9% ...running twitter processing\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Preprocessing\n",
    "\"\"\"\n",
    "print(np.shape(df_raw_twitter_data))\n",
    "corpus = []\n",
    "print(\"\")\n",
    "time = 0\n",
    "for index,[sentiment, tweet] in enumerate(df_raw_twitter_data):\n",
    "    time += 1\n",
    "    # Tokenize Words \n",
    "    tokens = tweet.split(\" \")\n",
    "    # Remove Links, @ mentions, # tags\n",
    "    links = ['http', '.com', '#', '@', '&', '~']\n",
    "    tokens = [w for w in tokens if not any(x in w for x in links)]\n",
    "    # Tokenize again \n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    tokens = [regex.sub('', w) for w in tokens]\n",
    "    # Lemmatization \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens ]\n",
    "    # Clean Up\n",
    "    tokens = [w.lower() for w in tokens if len(w) > 0]\n",
    "    # Remove Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('im')\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    corpus.append(\" \".join(tokens))\n",
    "    if time % 100 == 0:\n",
    "        progress(time,99988,suffix=\"running twitter processing\")\n",
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished vectorization\n",
      "(5000, 7882)\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "star = 5000\n",
    "##############\n",
    "corpus = np.load('corpus.npy')\n",
    "vectorizer = CountVectorizer()\n",
    "vobj = vectorizer.fit_transform(corpus[:star])\n",
    "vectors = vobj.toarray()\n",
    "print(\"\\nFinished vectorization\")\n",
    "print(np.shape(vectors))\n",
    "np.save('vectors.npy', vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a StandardScaler object to 0 mean the data matrix but preserve the variance\n",
    "stand_scaler = sklpp.StandardScaler(with_mean = True, with_std = False)\n",
    "# Fits the data matrix to the StandardScaler object defined ^\n",
    "centered_ImageData = stand_scaler.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a PCA object that reduces the dimensions of our data matrix keeping 95% of the variance\n",
    "pca_obj = skldecomp.PCA(n_components = 0.90, svd_solver = 'auto')\n",
    "dim_reducedImageData = pca_obj.fit_transform(centered_ImageData)\n",
    "np.save('twitterDataReduced.npy', dim_reducedImageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been reduced to 1719 features after PCA\n"
     ]
    }
   ],
   "source": [
    "print('Data has been reduced to {} features after PCA'.format(dim_reducedImageData.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDA: Justin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on k=1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "\"\"\"\n",
    "Pull pca data from saved\n",
    "\"\"\"\n",
    "#data = np.load('twitterDataReduced.npy')\n",
    "data = vectors\n",
    "labels = labels[:5000]\n",
    "number_of_parameters = len(data[0]) - 1 #5000\n",
    "number_of_total_samples = len(data)\n",
    "number_of_labels = 2\n",
    "\"\"\"\n",
    "K-Fold cross validation \n",
    "\"\"\"     \n",
    "final_estimates = []\n",
    "k = 10\n",
    "for i in range(1,11):\n",
    "    training_data = np.concatenate((data[0:(i-1)*int(number_of_total_samples/k)],data[i*int(number_of_total_samples/k):number_of_total_samples]),axis=0)\n",
    "    training_labels = np.concatenate((labels[0:(i-1)*int(number_of_total_samples/k)],labels[i*int(number_of_total_samples/k):number_of_total_samples]),axis=0)\n",
    "    testing_data = data[(i-1)*int(number_of_total_samples/k):i*int(number_of_total_samples/k)]\n",
    "    number_of_training_samples = len(training_data)\n",
    "    clf = QDA()\n",
    "    print(\"Training on k=\"+str(i))\n",
    "    clf.fit(training_data, training_labels)\n",
    "    print(\"\")\n",
    "    correct = 0\n",
    "    for index, image in enumerate(testing_data):\n",
    "        if index % 1 == 0:\n",
    "            progress(index+1,int(star/10),suffix=\"testing k= \"+str(i))\n",
    "        if int(testing_data[index][-1]) == int(clf.predict([image])[0]):\n",
    "            correct += 1\n",
    "    sys.stdout.flush()\n",
    "    print(\"\")\n",
    "    final_estimates.append(correct/int(star/10))\n",
    "print(final_estimates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pull pca data from saved\n",
    "\"\"\"\n",
    "data = np.load('twitterDataReduced.npy')\n",
    "data = np.concatenate((data,labels[:star]),axis=1)\n",
    "number_of_parameters = len(data[0]) - 1 #5000\n",
    "number_of_total_samples = len(data)\n",
    "number_of_labels = 2\n",
    "\"\"\"\n",
    "K-Fold cross validation \n",
    "\"\"\"     \n",
    "final_estimates = []\n",
    "k = 10\n",
    "def QDA(i):\n",
    "    training_data = np.concatenate((data[0:(i-1)*int(number_of_total_samples/k)],data[i*int(number_of_total_samples/k):number_of_total_samples]),axis=0)\n",
    "    testing_data = data[(i-1)*int(number_of_total_samples/k):i*int(number_of_total_samples/k)]\n",
    "    number_of_training_samples = len(training_data)\n",
    "    \"\"\"\n",
    "    Define Parameters \n",
    "    \"\"\"\n",
    "    mu = [[0 for j in range(number_of_parameters)] for i in range(number_of_labels)] # initializing means \n",
    "    sigmas = [[[0 for k in range(number_of_parameters)] for j in range(number_of_parameters)] for i in range(number_of_labels)] # initializing variances\n",
    "    occurences = [0 for i in range(10)]\n",
    "    pi = [0 for i in range(10)]\n",
    "        \n",
    "    \"\"\"\n",
    "    Learn mu\n",
    "    \"\"\"\n",
    "    # Find Sums and Occurences\n",
    "    for image in training_data:\n",
    "        label = int(image[-1])\n",
    "        occurences[label] += 1\n",
    "        mu[label] = np.add(mu[label],image[:-1])\n",
    "    # Calculate Averages and Prior Probabilities \n",
    "    for label,sums in enumerate(mu):\n",
    "        mu[label] = np.multiply(1/(occurences[label] - 1), sums).reshape(-1,1) #unbaised estimator\n",
    "        pi[label] = occurences[label] / number_of_total_samples\n",
    "    \"\"\"\n",
    "    Learn sigma^2\n",
    "    \"\"\"\n",
    "    time = 0\n",
    "    for image in training_data:\n",
    "        time += 1\n",
    "        if time%10 == 0:\n",
    "            progress(time,number_of_training_samples,suffix=\"training k= \"+str(i))\n",
    "        label = int(image[-1])\n",
    "        image = image[:-1].reshape(-1,1)\n",
    "        difference = np.subtract(image,mu[label])\n",
    "        sigma = difference.dot(difference.T)\n",
    "        sigmas[label] = np.add(sigmas[label],sigma)\n",
    "    for label,sigma in enumerate(sigmas):\n",
    "        sigmas[label] = np.multiply(1/(occurences[label]-1),sigma).astype(np.float32)\n",
    "    sys.stdout.flush()\n",
    "    \"\"\"\n",
    "    Method to find the best discriminant score\n",
    "    \"\"\"\n",
    "    def estimateBestLabel(image):\n",
    "        scores = [0 for _ in range(number_of_labels)]\n",
    "        # find score for each label \n",
    "\n",
    "        for label in range(number_of_labels):\n",
    "            inverted_variance = np.linalg.inv(sigmas[label])\n",
    "            first_term = -0.5*image.T.dot(inverted_variance).dot(image)\n",
    "            second_term = image.T.dot(inverted_variance).dot(mu[label])\n",
    "            third_term = -0.5*mu[label].T.dot(inverted_variance).dot(mu[label])\n",
    "            (sign, logdet) = np.linalg.slogdet(sigmas[label])\n",
    "            fourth_term = -0.5*sign*logdet\n",
    "            fifth_term = math.log(pi[label])\n",
    "            score = first_term + second_term + third_term + fourth_term + fifth_term\n",
    "            scores[label] = score[0][0]\n",
    "        return(scores.index(max(scores)))\n",
    "    correct = 0\n",
    "    print(\"\")\n",
    "    for index, image in enumerate(testing_data):\n",
    "        if index % 1 == 0:\n",
    "            progress(index+1,int(star/10),suffix=\"testing k= \"+str(i))\n",
    "        image = image[:-1].reshape(-1,1)\n",
    "        if int(testing_data[index][-1]) == int(estimateBestLabel(image)):\n",
    "            correct += 1\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    print(correct/(int(star/10)))\n",
    "    final_estimates.append(correct/int(star/10))\n",
    "print(\"\")\n",
    "print(final_estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"*\n",
    "K = 1 & 2\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "QDA(1)\n",
    "end = time.time()\n",
    "print((end-start)/60)\n",
    "\n",
    "start = time.time()\n",
    "QDA(2)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"*\n",
    "K = 3 & 4\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "QDA(3)\n",
    "end = time.time()\n",
    "print((end-start)/60)\n",
    "\n",
    "start = time.time()\n",
    "QDA(4)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...training k= 5\n",
      "0.45=========================================================] 100.0% ...testing k= 5\n",
      "41.58777117729187\n",
      "[============================================================] 100.0% ...training k= 6\n",
      "0.748========================================================] 100.0% ...testing k= 6\n",
      "41.84258098602295\n"
     ]
    }
   ],
   "source": [
    "\"\"\"*\n",
    "K = 5 & 6\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "QDA(5)\n",
    "end = time.time()\n",
    "print((end-start)/60)\n",
    "\n",
    "start = time.time()\n",
    "QDA(6)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...training k= 7\n",
      "0.572========================================================] 100.0% ...testing k= 7\n",
      "41.69866884152095\n",
      "[============================================================] 100.0% ...training k= 8\n",
      "0.346========================================================] 100.0% ...testing k= 8\n",
      "39.27590667009353\n"
     ]
    }
   ],
   "source": [
    "\"\"\"*\n",
    "K = 7 & 8\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "QDA(7)\n",
    "end = time.time()\n",
    "print((end-start)/60)\n",
    "start = time.time()\n",
    "\n",
    "start = time.time()\n",
    "QDA(8)\n",
    "end = time.time()\n",
    "print((end-start)/60)\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...training k= 9\n",
      "[------------------------------------------------------------] 0.6% ...testing k= 9\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-4e82e31b2055>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mQDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e8e092798264>\u001b[0m in \u001b[0;36mQDA\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mprogress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstar\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"testing k= \"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimateBestLabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-e8e092798264>\u001b[0m in \u001b[0;36mestimateBestLabel\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0minverted_variance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mfirst_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverted_variance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0msecond_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverted_variance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mthird_term\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minverted_variance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msign\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslogdet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigmas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"*\n",
    "K = 9 & 10\n",
    "\"\"\"\n",
    "start = time.time()\n",
    "QDA(9)\n",
    "end = time.time()\n",
    "print((end-start)/60)\n",
    "\n",
    "start = time.time()\n",
    "QDA(10)\n",
    "end = time.time()\n",
    "print((end-start)/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "[============================================================] 100.0% ...training k= 1\n",
    "0.656========================================================] 100.0% ...testing k= 1\n",
    "[============================================================] 100.0% ...training k= 2\n",
    "0.474========================================================] 100.0% ...testing k= 2\n",
    "-2413.8574686050415\n",
    "[============================================================] 100.0% ...training k= 3\n",
    "0.654========================================================] 100.0% ...testing k= 3\n",
    "43.156752200921375\n",
    "[============================================================] 100.0% ...training k= 4\n",
    "0.354========================================================] 100.0% ...testing k= 4\n",
    "43.58165429830551\n",
    "[============================================================] 100.0% ...training k= 5\n",
    "0.55=========================================================] 100.0% ...testing k= 5\n",
    "41.879090480009715\n",
    "[============================================================] 100.0% ...training k= 6\n",
    "[==========================================================--] 96.4% ...testing k= 6\n",
    "[============================================================] 100.0% ...training k= 7\n",
    "0.428========================================================] 100.0% ...testing k= 7\n",
    "42.374602814515434\n",
    "[============================================================] 100.0% ...training k= 8\n",
    "0.654========================================================] 100.0% ...testing k= 8\n",
    "42.34428945382436\n",
    "[============================================================] 100.0% ...training k= 9\n",
    "0.37=========================================================] 100.0% ...testing k= 9\n",
    "42.841961006323494\n",
    "[==================================================----------] 82.7% ...training k= 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Opening File, creating nparray \n",
    "Data is already 0 mean 1 variance \n",
    "\"\"\"\n",
    "features = open('human_activity_features_train_data.txt','r')\n",
    "human_activity_data = None\n",
    "for feature in features:\n",
    "    feature = [float(w) for w in features.readline().split(\" \") if len(w) > 0]\n",
    "    try:\n",
    "        human_activity_data = np.concatenate((data, feature), axis=0)\n",
    "    except:\n",
    "        human_activity_data = feature\n",
    "features.close()\n",
    "print(np.shape(human_activity_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a StandardScaler object to 0 mean the data matrix but preserve the variance\n",
    "stand_scaler = sklpp.StandardScaler(with_mean = True, with_std = False)\n",
    "# Fits the data matrix to the StandardScaler object defined ^\n",
    "centered_ImageData = stand_scaler.fit_transform(human_activity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a PCA object that reduces the dimensions of our data matrix keeping 95% of the variance\n",
    "pca_obj = skldecomp.PCA(n_components = 0.90, svd_solver = 'auto')\n",
    "dim_reducedImageData = pca_obj.fit_transform(centered_ImageData)\n",
    "np.save('dim_reducedImageData.npy', dim_reducedImageData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical/Agglomerative Clustering (Justin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of clustering breaks all data points down into centroids and groups them one by one until it reaches the specified number of clusters, k. The linkage policy determines grouping, which are:\n",
    " - simple: closest distance between clusters \n",
    " - complete: farthest distance between clusters \n",
    " - average: average distance between clusters \n",
    " - ward: sum of squared differences\n",
    "Our implementation is using euclidean distance \n",
    "\n",
    "Hierarchical/Agglomerative is deterministic and⁠—as compared to k-means⁠—is slow. Complete, Average, and Ward linkage policies yield a $n^{3}$ runtime. Simple linkage yields $n^{2}$ runtime with clever optimizations, which is why we are using sklearn. \n",
    "\n",
    "We are using k=6 because of our a-priori knoweldge that there are 6 groups:\n",
    " 1. WALKING,\n",
    " 2. WALKING_UPSTAIRS,\n",
    " 3. WALKING_DOWNSTAIRS,\n",
    " 4. SITTING,\n",
    " 5. STANDING,\n",
    " 6. LAYING;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering(n_clusters = 6, linkage='single').fit(human_activity_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
