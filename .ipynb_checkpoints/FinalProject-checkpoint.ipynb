{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Final Project\n",
    "\n",
    "Justin May, Joe Shenouda, Jonathan Hong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Import Statements\n",
    "\"\"\"\n",
    "# Python Specific \n",
    "import pickle \n",
    "import time\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Data Science \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing as sklpp\n",
    "from sklearn import decomposition as skldecomp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import csv\n",
    "import warnings\n",
    "import re # for regular expressions\n",
    "\n",
    "# NLP Libraries -- for twitter data preprocessing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords # A set of the most common stop words \n",
    "from nltk.stem import WordNetLemmatizer # A function to stem words \n",
    "from nltk.tokenize import word_tokenize # An auto-tokenizer \n",
    "# ---- RUN THESE IF RUNNING FOR THE FIRST TIME ----\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# Options\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper Function to See Progress \n",
    "\"\"\"\n",
    "def progress(count, total, suffix=''):\n",
    "    bar_len = 60\n",
    "    filled_len = int(round(bar_len * count / float(total)))\n",
    "\n",
    "    percents = round(100.0 * count / float(total), 1)\n",
    "    bar = '=' * filled_len + '-' * (bar_len - filled_len)\n",
    "\n",
    "    sys.stdout.write('[%s] %s%s ...%s\\r' % (bar, percents, '%', suffix))\n",
    "    sys.stdout.flush()  # As suggested by Rom Ruben"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Images - Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting file images/data_batch_1...\n",
      "Finished Extracting Features\n",
      "(10000, 3072)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extracting data into a file\n",
    "Data are all in the same domain, no need to normalize\n",
    "\"\"\"\n",
    "data = None\n",
    "labels = []\n",
    "\n",
    "file = \"images/data_batch_1\"\n",
    "with open(file, 'rb') as fo:\n",
    "    print(\"extracting file \"+file+\"...\")\n",
    "    dict = pickle.load(fo, encoding='bytes')\n",
    "    temp_data = dict[b'data']\n",
    "    try:\n",
    "        data = np.concatenate((data, temp_data), axis=0)\n",
    "    except:\n",
    "        data = temp_data\n",
    "    labels = labels + dict[b'labels']\n",
    "labels = np.array(labels)\n",
    "labels = labels.reshape(-1,1)\n",
    "print(\"Finished Extracting Features\")\n",
    "print(np.shape(data))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing and Feature Learning For Images Dataset\n",
    "For preprocessing we centered our dataset to 0 mean and then for feature learning we apply PCA to reduce our dimensions from 3072 to a smaller number of features than contains 95% of the variance of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a StandardScaler object to 0 mean the data matrix but preserve the variance\n",
    "stand_scaler = sklpp.StandardScaler(with_mean = True, with_std = False)\n",
    "# Fits the data matrix to the StandardScaler object defined ^\n",
    "centered_ImageData = stand_scaler.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a PCA object that reduces the dimensions of our data matrix keeping 95% of the variance\n",
    "pca_obj = skldecomp.PCA(n_components = 0.95, svd_solver = 'auto')\n",
    "dim_reducedImageData = pca_obj.fit_transform(centered_ImageData)\n",
    "np.save('dim_reducedImageData.npy', dim_reducedImageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been reduced to 209 features after PCA\n"
     ]
    }
   ],
   "source": [
    "print('Data has been reduced to {} features after PCA'.format(dim_reducedImageData.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Model 1) Quadratic Discriminant Analysis: Justin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I implemeneted the algorithim we learned in class: \n",
    " - first learn the parameters (expected average, prior probabilities, covariance matricies) \n",
    " - define the discriminant functions to find the best labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[============================================================] 100.0% ...training k= 1\n",
      "0.466========================================================] 100.0% ...testing k= 1\n",
      "[============================================================] 100.0% ...training k= 2\n",
      "0.498========================================================] 100.0% ...testing k= 2\n",
      "[============================================================] 100.0% ...training k= 3\n",
      "0.479========================================================] 100.0% ...testing k= 3\n",
      "[============================================================] 100.0% ...training k= 4\n",
      "0.473========================================================] 100.0% ...testing k= 4\n",
      "[============================================================] 100.0% ...training k= 5\n",
      "0.464========================================================] 100.0% ...testing k= 5\n",
      "[============================================================] 100.0% ...training k= 6\n",
      "0.49=========================================================] 100.0% ...testing k= 6\n",
      "[============================================================] 100.0% ...training k= 7\n",
      "0.441========================================================] 100.0% ...testing k= 7\n",
      "[============================================================] 100.0% ...training k= 8\n",
      "0.469========================================================] 100.0% ...testing k= 8\n",
      "[============================================================] 100.0% ...training k= 9\n",
      "0.482========================================================] 100.0% ...testing k= 9\n",
      "[============================================================] 100.0% ...training k= 10\n",
      "0.473========================================================] 100.0% ...testing k= 10\n",
      "\n",
      "[0.466, 0.498, 0.479, 0.473, 0.464, 0.49, 0.441, 0.469, 0.482, 0.473]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Pull pca data from saved\n",
    "\"\"\"\n",
    "data = np.load('dim_reducedImageData.npy')\n",
    "data = np.concatenate((data,labels),axis=1)\n",
    "number_of_parameters = len(data[0]) - 1 #209\n",
    "number_of_total_samples = len(data)\n",
    "\"\"\"\n",
    "K-Fold cross validation \n",
    "\"\"\"     \n",
    "final_estimates = []\n",
    "k = 10\n",
    "for i in range(1,k+1):\n",
    "    training_data = np.concatenate((data[0:(i-1)*int(number_of_total_samples/k)],data[i*int(number_of_total_samples/k):number_of_total_samples]),axis=0)\n",
    "    testing_data = data[(i-1)*int(number_of_total_samples/k):i*int(number_of_total_samples/k)]\n",
    "    number_of_training_samples = len(training_data)\n",
    "    \"\"\"\n",
    "    Define Parameters \n",
    "    \"\"\"\n",
    "    mu = [[0 for j in range(number_of_parameters)] for i in range(10)] # initializing means \n",
    "    sigmas = [[[0 for k in range(number_of_parameters)] for j in range(number_of_parameters)] for i in range(10)] # initializing variances\n",
    "    occurences = [0 for i in range(10)]\n",
    "    pi = [0 for i in range(10)]\n",
    "        \n",
    "    \"\"\"\n",
    "    Learn mu\n",
    "    \"\"\"\n",
    "    # Find Sums and Occurences\n",
    "    for image in training_data:\n",
    "        label = int(image[-1])\n",
    "        occurences[label] += 1\n",
    "        mu[label] = np.add(mu[label],image[:-1])\n",
    "    # Calculate Averages and Prior Probabilities \n",
    "    for label,sums in enumerate(mu):\n",
    "        mu[label] = np.multiply(1/(occurences[label] - 1), sums).reshape(-1,1) #unbaised estimator\n",
    "        pi[label] = occurences[label] / number_of_total_samples\n",
    "    \"\"\"\n",
    "    Learn sigma^2\n",
    "    \"\"\"\n",
    "    time = 0\n",
    "    for image in training_data:\n",
    "        time += 1\n",
    "        if time%10 == 0:\n",
    "            progress(time,number_of_training_samples,suffix=\"training k= \"+str(i))\n",
    "        label = int(image[-1])\n",
    "        image = image[:-1].reshape(-1,1)\n",
    "        difference = np.subtract(image,mu[label])\n",
    "        sigma = difference.dot(difference.T)\n",
    "        sigmas[label] = np.add(sigmas[label],sigma)\n",
    "    for label,sigma in enumerate(sigmas):\n",
    "        sigmas[label] = np.multiply(1/(occurences[label]-1),sigma)\n",
    "    sys.stdout.flush()\n",
    "    \"\"\"\n",
    "    Method to find the best discriminant score\n",
    "    \"\"\"\n",
    "    def estimateBestLabel(image):\n",
    "        scores = [0 for _ in range(10)]\n",
    "        # find score for each label \n",
    "\n",
    "        for label in range(10):\n",
    "            inverted_variance = np.linalg.inv(sigmas[label])\n",
    "            first_term = -0.5*image.T.dot(inverted_variance).dot(image)\n",
    "            second_term = image.T.dot(inverted_variance).dot(mu[label])\n",
    "            third_term = -0.5*mu[label].T.dot(inverted_variance).dot(mu[label])\n",
    "            (sign, logdet) = np.linalg.slogdet(sigmas[label])\n",
    "            fourth_term = -0.5*sign*logdet\n",
    "            fifth_term = math.log(pi[label])\n",
    "            score = first_term + second_term + third_term + fourth_term + fifth_term\n",
    "            scores[label] = score[0][0]\n",
    "        return(scores.index(max(scores)))\n",
    "    correct = 0\n",
    "    print(\"\")\n",
    "    for index, image in enumerate(testing_data):\n",
    "        if index % 9 == 0:\n",
    "            progress(index+1,1000,suffix=\"testing k= \"+str(i))\n",
    "        image = image[:-1].reshape(-1,1)\n",
    "        if testing_data[index][-1] == estimateBestLabel(image):\n",
    "            correct += 1\n",
    "    sys.stdout.flush()\n",
    "    print(correct/1000)\n",
    "    final_estimates.append(correct/1000)\n",
    "print(\"\")\n",
    "print(final_estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results \n",
    "\n",
    "### QDA\n",
    "[0.466, 0.498, 0.479, 0.473, 0.464, 0.49, 0.441, 0.469, 0.482, 0.473]\n",
    "\n",
    "### Logistics Regression \n",
    "\n",
    "### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data - Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our twitter data preprocessing step consistions of the following: \n",
    "   1. Tokenizing words the tweets \n",
    "   2. Removing any links, @ mentions, and #tags that don't have semantic meaning outside of twitter\n",
    "   3. Lemmatization: remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma\n",
    "   4. Lower case all words \n",
    "   5. Remove stopwords: a commonly used word (such as “the”, “a”, “an”, “in”) that have little semantic meaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reading in Data\n",
    "\"\"\"\n",
    "df_raw_twitter_data = pd.read_csv('train.csv', header=None, encoding = \"ISO-8859-1\").values[1:]\n",
    "df_raw_twitter_data = df_raw_twitter_data[:,1:] # getting rid of index \n",
    "\n",
    "labels = df_raw_twitter_data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99989, 2)\n",
      "\n",
      "[============================================================] 99.9% ...running twitter processing\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Preprocessing\n",
    "\"\"\"\n",
    "print(np.shape(df_raw_twitter_data))\n",
    "corpus = []\n",
    "print(\"\")\n",
    "time = 0\n",
    "for index,[sentiment, tweet] in enumerate(df_raw_twitter_data):\n",
    "    time += 1\n",
    "    # Tokenize Words \n",
    "    tokens = tweet.split(\" \")\n",
    "    # Remove Links, @ mentions, # tags\n",
    "    links = ['http', '.com', '#', '@', '&', '~']\n",
    "    tokens = [w for w in tokens if not any(x in w for x in links)]\n",
    "    # Tokenize again \n",
    "    regex = re.compile('[^a-zA-Z]')\n",
    "    tokens = [regex.sub('', w) for w in tokens]\n",
    "    # Lemmatization \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tokens = [lemmatizer.lemmatize(w) for w in tokens ]\n",
    "    # Clean Up\n",
    "    tokens = [w.lower() for w in tokens if len(w) > 0]\n",
    "    # Remove Stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stop_words.add('im')\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    corpus.append(\" \".join(tokens))\n",
    "    if time % 100 == 0:\n",
    "        progress(time,99988,suffix=\"running twitter processing\")\n",
    "sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizing the Words \n",
    "CountVectorizer() is a sklearn library that turns a list of strings into a $n\\mathbb{x}p$ array. Each row corresponds to a sample $x_{i}$ and each column corresponds to a unique word. The value is the occurence of the word in the tweet. We are using a bag-of-words approach, the simplest approach.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Finished vectorization\n",
      "(5000, 7882)\n"
     ]
    }
   ],
   "source": [
    "##############\n",
    "star = 5000\n",
    "##############\n",
    "corpus = np.load('corpus.npy')\n",
    "vectorizer = CountVectorizer()\n",
    "vobj = vectorizer.fit_transform(corpus[:star])\n",
    "vectors = vobj.toarray()\n",
    "print(\"\\nFinished vectorization\")\n",
    "print(np.shape(vectors))\n",
    "np.save('corpus.npy', vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a StandardScaler object to 0 mean the data matrix but preserve the variance\n",
    "stand_scaler = sklpp.StandardScaler(with_mean = True, with_std = False)\n",
    "# Fits the data matrix to the StandardScaler object defined ^\n",
    "centered_ImageData = stand_scaler.fit_transform(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a PCA object that reduces the dimensions of our data matrix keeping 95% of the variance\n",
    "pca_obj = skldecomp.PCA(n_components = 0.90, svd_solver = 'auto')\n",
    "dim_reducedImageData = pca_obj.fit_transform(centered_ImageData)\n",
    "np.save('twitterDataReduced.npy', dim_reducedImageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been reduced to 1719 features after PCA\n"
     ]
    }
   ],
   "source": [
    "print('Data has been reduced to {} features after PCA'.format(dim_reducedImageData.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QDA: Justin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### QDA Implementation ###\n",
    "With this QDA implementation I chose to go with sklearn. Sklearn's behind the scenes optimizations are simply 100x better than mine. Literally, it took around 43 minutes on average to train on each k with n=5000 and p=1719. I probably spent around 14+ hours playing around with optimizing my code and finally just tried sklearn's qda implentation. It finished in less than 5 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on k=1\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 1\n",
      "Training on k=2\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 2\n",
      "Training on k=3\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 3\n",
      "Training on k=4\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 4\n",
      "Training on k=5\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 5\n",
      "Training on k=6\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 6\n",
      "Training on k=7\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 7\n",
      "Training on k=8\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 8\n",
      "Training on k=9\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 9\n",
      "Training on k=10\n",
      "\n",
      "[============================================================] 100.0% ...testing k= 10\n",
      "[1.0, 0.988, 0.988, 0.994, 0.976, 0.99, 0.98, 0.976, 0.958, 0.962]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "\"\"\"\n",
    "Pull pca data from saved\n",
    "\"\"\"\n",
    "data = np.load('twitterDataReduced.npy')\n",
    "labels = labels[:5000]\n",
    "number_of_parameters = len(data[0]) - 1 #5000\n",
    "number_of_total_samples = len(data)\n",
    "number_of_labels = 2\n",
    "\"\"\"\n",
    "K-Fold cross validation \n",
    "\"\"\"     \n",
    "final_estimates = []\n",
    "k = 10\n",
    "for i in range(1,11):\n",
    "    training_data = np.concatenate((data[0:(i-1)*int(number_of_total_samples/k)],data[i*int(number_of_total_samples/k):number_of_total_samples]),axis=0)\n",
    "    training_labels = np.concatenate((labels[0:(i-1)*int(number_of_total_samples/k)],labels[i*int(number_of_total_samples/k):number_of_total_samples]),axis=0)\n",
    "    testing_data = data[(i-1)*int(number_of_total_samples/k):i*int(number_of_total_samples/k)]\n",
    "    number_of_training_samples = len(training_data)\n",
    "    clf = QDA()\n",
    "    print(\"Training on k=\"+str(i))\n",
    "    clf.fit(training_data, training_labels)\n",
    "    print(\"\")\n",
    "    correct = 0\n",
    "    for index, image in enumerate(testing_data):\n",
    "        if index % 1 == 0:\n",
    "            progress(index+1,int(star/10),suffix=\"testing k= \"+str(i))\n",
    "        if int(testing_data[index][-1]) == int(clf.predict([image])[0]):\n",
    "            correct += 1\n",
    "    sys.stdout.flush()\n",
    "    print(\"\")\n",
    "    final_estimates.append(correct/int(star/10))\n",
    "print(final_estimates)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human Activity Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features:  (3676, 561)\n",
      "labels:  (3676,)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Opening File, creating nparray \n",
    "Data is already 0 mean 1 variance \n",
    "\"\"\"\n",
    "features = open('human_activity_features_train_data.txt','r')\n",
    "human_activity_data = []\n",
    "for feature in features:\n",
    "    feature = np.array([float(w) for w in features.readline().split(\" \") if len(w) > 0])\n",
    "    human_activity_data.append(feature)\n",
    "features.close()\n",
    "human_activity_data = np.array(human_activity_data)\n",
    "print(\"features: \",np.shape(human_activity_data))\n",
    "\n",
    "labels = open('human_activity_labels_train_data.txt','r')\n",
    "human_activity_labels = []\n",
    "for label in labels:\n",
    "    human_activity_labels.append(int(label))\n",
    "labels.close()\n",
    "print(\"labels: \",np.shape(human_activity_labels[:3676]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we create a StandardScaler object to 0 mean the data matrix but preserve the variance\n",
    "stand_scaler = sklpp.StandardScaler(with_mean = True, with_std = False)\n",
    "# Fits the data matrix to the StandardScaler object defined ^\n",
    "centered_ImageData = stand_scaler.fit_transform(human_activity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a PCA object that reduces the dimensions of our data matrix keeping 95% of the variance\n",
    "pca_obj = skldecomp.PCA(n_components = 0.90, svd_solver = 'auto')\n",
    "dim_reducedImageData = pca_obj.fit_transform(centered_ImageData)\n",
    "np.save('human_activity_data.npy', dim_reducedImageData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been reduced to 34 features after PCA\n"
     ]
    }
   ],
   "source": [
    "print('Data has been reduced to {} features after PCA'.format(dim_reducedImageData.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical/Agglomerative Clustering (Justin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This type of clustering breaks all data points down into centroids and groups them one by one until it reaches the specified number of clusters, k. The linkage policy determines grouping, which are:\n",
    " - simple: closest distance between clusters \n",
    " - complete: farthest distance between clusters \n",
    " - average: average distance between clusters \n",
    " - ward: sum of squared differences\n",
    "Our implementation is using euclidean distance \n",
    "\n",
    "Hierarchical/Agglomerative is deterministic and⁠—as compared to k-means⁠—is slow. Complete, Average, and Ward linkage policies yield a $n^{3}$ runtime. Simple linkage yields $n^{2}$ runtime with clever optimizations, which is why we are using sklearn. \n",
    "\n",
    "We are using k=6 because of our a-priori knoweldge that there are 6 groups:\n",
    " 1. WALKING,\n",
    " 2. WALKING_UPSTAIRS,\n",
    " 3. WALKING_DOWNSTAIRS,\n",
    " 4. SITTING,\n",
    " 5. STANDING,\n",
    " 6. LAYING;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "human_activity_data = np.load('human_activity_data.npy')\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "clustering = AgglomerativeClustering(n_clusters = 6, linkage='ward').fit(human_activity_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3676,)\n"
     ]
    }
   ],
   "source": [
    "clustering.labels_\n",
    "print(np.shape(clustering.labels_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
